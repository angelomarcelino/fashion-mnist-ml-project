{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_RBKS9fnYUfn"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1752151347975,
     "user": {
      "displayName": "Angelo Marcelino",
      "userId": "12130434257979385717"
     },
     "user_tz": 180
    },
    "id": "73VdrnHEVdyU"
   },
   "outputs": [],
   "source": [
    "# Import standard libraries for randomness, deep copying, and numerical operations\n",
    "import random\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "# Import libraries for image processing and data manipulation\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "# Import PyTorch core and utilities for deep learning\n",
    "import torch\n",
    "import torch.optim as optim  # Optimization algorithms\n",
    "import torch.nn as nn  # Neural network modules\n",
    "import torch.nn.functional as F  # Functional API for non-parametric operations\n",
    "\n",
    "# Import PyTorch utilities for data loading and transformations\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, WeightedRandomSampler\n",
    "from torchvision.transforms.v2 import Compose, ToImage, Normalize, ToPILImage, Resize, ToDtype\n",
    "\n",
    "# Import dataset handling and learning rate schedulers\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, MultiStepLR, CyclicLR, LambdaLR\n",
    "\n",
    "# Import visualization and web utilities\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "import errno\n",
    "\n",
    "# Set matplotlib style for better visuals\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZSech2uFZZVj"
   },
   "source": [
    "# Architecture class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 250,
     "status": "ok",
     "timestamp": 1752151348266,
     "user": {
      "displayName": "Angelo Marcelino",
      "userId": "12130434257979385717"
     },
     "user_tz": 180
    },
    "id": "lWS3VTALZOqM"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class Architecture(object):\n",
    "    def __init__(self, model, loss_fn, optimizer, verbose=True, class_names=None):\n",
    "        # Here we define the attributes of our class\n",
    "        self.verbose=verbose\n",
    "        self.class_names = class_names\n",
    "\n",
    "        # We start by storing the arguments as attributes\n",
    "        # to use them later\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        # Let's send the model to the specified device right away\n",
    "        self.model.to(self.device)\n",
    "        if self.verbose:\n",
    "            print(f\"Model sent to {self.device}\")\n",
    "\n",
    "        # These attributes are defined here, but since they are\n",
    "        # not informed at the moment of creation, we keep them None\n",
    "        self.train_loader = None\n",
    "        self.val_loader = None\n",
    "\n",
    "        # These attributes are going to be computed internally\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        self.total_epochs = 0\n",
    "\n",
    "        # Creates the train_step function for our model,\n",
    "        # loss function and optimizer\n",
    "        # Note: there are NO ARGS there! It makes use of the class\n",
    "        # attributes directly\n",
    "        self.train_step_fn = self._make_train_step_fn()\n",
    "        # Creates the val_step function for our model and loss\n",
    "        self.val_step_fn = self._make_val_step_fn()\n",
    "\n",
    "        # for hook purposes\n",
    "        self.handles = {}\n",
    "        self.visualization = {}\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"Architecture created\")\n",
    "\n",
    "    def to(self, device):\n",
    "        # This method allows the user to specify a different device\n",
    "        # It sets the corresponding attribute (to be used later in\n",
    "        # the mini-batches) and sends the model to the device\n",
    "        try:\n",
    "            self.device = device\n",
    "            self.model.to(self.device)\n",
    "            if self.verbose:\n",
    "                print(f\"Model sent to {device}\")\n",
    "        except RuntimeError:\n",
    "            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "            print(f\"Couldn't send it to {device}, sending it to {self.device} instead.\")\n",
    "            self.model.to(self.device)\n",
    "\n",
    "    def set_loaders(self, train_loader, val_loader=None):\n",
    "        # This method allows the user to define which train_loader (and val_loader, optionally) to use\n",
    "        # Both loaders are then assigned to attributes of the class\n",
    "        # So they can be referred to later\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "\n",
    "        if self.verbose:\n",
    "            # Print train loader info\n",
    "            print(\"Loaders set\")\n",
    "            if self.train_loader.dataset:\n",
    "                print(f\"Train dataset size: {len(self.train_loader.dataset)}\")\n",
    "            if self.train_loader.batch_size:\n",
    "                print(f\"Train batch size: {self.train_loader.batch_size}\")\n",
    "\n",
    "    def _make_train_step_fn(self):\n",
    "        # This method does not need ARGS... it can refer to\n",
    "        # the attributes: self.model, self.loss_fn and self.optimizer\n",
    "\n",
    "        # Builds function that performs a step in the train loop\n",
    "        def perform_train_step_fn(x, y):\n",
    "            # Sets model to TRAIN mode\n",
    "            self.model.train()\n",
    "\n",
    "            # Step 1 - Computes our model's predicted output - forward pass\n",
    "            yhat = self.model(x)\n",
    "            # Step 2 - Computes the loss\n",
    "            loss = self.loss_fn(yhat, y)\n",
    "            # Step 3 - Computes gradients for both \"a\" and \"b\" parameters\n",
    "            loss.backward()\n",
    "            # Step 4 - Updates parameters using gradients and the learning rate\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # Returns the loss\n",
    "            return loss.item()\n",
    "\n",
    "        # Returns the function that will be called inside the train loop\n",
    "        return perform_train_step_fn\n",
    "\n",
    "    def _make_val_step_fn(self):\n",
    "        # Builds function that performs a step in the validation loop\n",
    "        def perform_val_step_fn(x, y):\n",
    "            # Sets model to EVAL mode\n",
    "            self.model.eval()\n",
    "\n",
    "            # Step 1 - Computes our model's predicted output - forward pass\n",
    "            yhat = self.model(x)\n",
    "            # Step 2 - Computes the loss\n",
    "            loss = self.loss_fn(yhat, y)\n",
    "            # There is no need to compute Steps 3 and 4, since we don't update parameters during evaluation\n",
    "            return loss.item()\n",
    "\n",
    "        return perform_val_step_fn\n",
    "\n",
    "    def _mini_batch(self, validation=False, verbose_mini_batch=None, mini_batch_report = 100):\n",
    "        # The mini-batch can be used with both loaders\n",
    "        # The argument `validation`defines which loader and\n",
    "        # corresponding step function is going to be used\n",
    "        if validation:\n",
    "            data_loader = self.val_loader\n",
    "            step_fn = self.val_step_fn\n",
    "        else:\n",
    "            data_loader = self.train_loader\n",
    "            step_fn = self.train_step_fn\n",
    "\n",
    "        if data_loader is None:\n",
    "            return None\n",
    "\n",
    "        local_verbose = verbose_mini_batch and self.verbose\n",
    "\n",
    "        if local_verbose:\n",
    "            print(\"\\tStarting mini-batch...\")\n",
    "\n",
    "        # Once the data loader and step function, this is the same\n",
    "        # mini-batch loop we had before\n",
    "        mini_batch_losses = []\n",
    "        count = 0\n",
    "        for x_batch, y_batch in data_loader:\n",
    "            x_batch = x_batch.to(self.device)\n",
    "            y_batch = y_batch.to(self.device)\n",
    "\n",
    "            mini_batch_loss = step_fn(x_batch, y_batch)\n",
    "            mini_batch_losses.append(mini_batch_loss)\n",
    "\n",
    "            if local_verbose and count % mini_batch_report == 0:\n",
    "                print(f\"\\t\\tBatch {count}/{len(data_loader)}, loss: {mini_batch_loss}\")\n",
    "\n",
    "        loss = np.mean(mini_batch_losses)\n",
    "        return loss\n",
    "\n",
    "    # this function was updated in this class\n",
    "    def set_seed(self, seed=42):\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        try:\n",
    "            self.train_loader.sampler.generator.manual_seed(seed)\n",
    "        except AttributeError:\n",
    "            pass\n",
    "        if self.verbose:\n",
    "            print(f\"Random seed set to {seed}\")\n",
    "\n",
    "    def train(self, n_epochs, seed=42, verbose=None, verbose_mini_batch=False, mini_batch_report=100, batch_report=10):\n",
    "        # To ensure reproducibility of the training process\n",
    "        self.set_seed(seed)\n",
    "\n",
    "        local_verbose = self.verbose if verbose is None else verbose\n",
    "        epoch_times = []\n",
    "\n",
    "        if local_verbose:\n",
    "            print(\"Starting training...\")\n",
    "            global_start = time.time()\n",
    "\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            # Keeps track of the numbers of epochs\n",
    "            # by updating the corresponding attribute\n",
    "            epoch_start = time.time()\n",
    "            self.total_epochs += 1\n",
    "\n",
    "            # inner loop\n",
    "            # Performs training using mini-batches\n",
    "            loss = self._mini_batch(validation=False,\n",
    "                                    verbose_mini_batch=verbose_mini_batch,\n",
    "                                    mini_batch_report=mini_batch_report)\n",
    "            self.losses.append(loss)\n",
    "\n",
    "            # VALIDATION\n",
    "            # no gradients in validation!\n",
    "            with torch.no_grad():\n",
    "                # Performs evaluation using mini-batches\n",
    "                val_loss = self._mini_batch(validation=True)\n",
    "                self.val_losses.append(val_loss)\n",
    "\n",
    "            epoch_end = time.time()\n",
    "            elapsed = epoch_end - epoch_start\n",
    "            epoch_times.append(elapsed)\n",
    "\n",
    "            is_first_epoch = epoch == 0\n",
    "            is_last_epoch = epoch == n_epochs - 1\n",
    "\n",
    "            has_report = is_first_epoch or is_last_epoch or (epoch + 1) % batch_report == 0\n",
    "\n",
    "            if local_verbose and has_report:\n",
    "                # Predict remaining duration\n",
    "                avg_time = sum(epoch_times) / len(epoch_times)\n",
    "                remaining_epochs = n_epochs - (epoch + 1)\n",
    "                estimated_remaining_secs = avg_time * remaining_epochs\n",
    "                mins, secs = divmod(int(estimated_remaining_secs), 60)\n",
    "\n",
    "                print(\n",
    "                    f\"Epoch {self.total_epochs}/{n_epochs} | \"\n",
    "                    f\"Train loss: {loss:.8f} | Val. loss: {val_loss:.8f} | \"\n",
    "                    f\"Time: {elapsed:.2f}s | ETA: {mins}m {secs}s\"\n",
    "                )\n",
    "\n",
    "        if local_verbose:\n",
    "            total_time = time.time() - global_start\n",
    "            mean_epoch_time = sum(epoch_times) / len(epoch_times)\n",
    "            print(f\"Training completed in {total_time:.2f} seconds.\")\n",
    "            print(f\"Mean time per epoch: {mean_epoch_time:.2f} seconds.\")\n",
    "\n",
    "    def save_checkpoint(self, filename):\n",
    "        # Builds dictionary with all elements for resuming training\n",
    "        checkpoint = {'epoch': self.total_epochs,\n",
    "                      'model_state_dict': self.model.state_dict(),\n",
    "                      'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                      'loss': self.losses,\n",
    "                      'val_loss': self.val_losses}\n",
    "\n",
    "        torch.save(checkpoint, filename)\n",
    "        if self.verbose:\n",
    "            print(f\"Checkpoint saved to {filename}\")\n",
    "\n",
    "    def load_checkpoint(self, filename):\n",
    "        # Loads dictionary\n",
    "        checkpoint = torch.load(filename)\n",
    "\n",
    "        # Restore state for model and optimizer\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "        self.total_epochs = checkpoint['epoch']\n",
    "        self.losses = checkpoint['loss']\n",
    "        self.val_losses = checkpoint['val_loss']\n",
    "\n",
    "        self.model.train() # always use TRAIN for resuming training\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"Checkpoint loaded from {filename}\")\n",
    "\n",
    "    def predict(self, x):\n",
    "        # Set is to evaluation mode for predictions\n",
    "        self.model.eval()\n",
    "        # Takes aNumpy input and make it a float tensor\n",
    "        x_tensor = torch.as_tensor(x).float()\n",
    "        # Send input to device and uses model for prediction\n",
    "        y_hat_tensor = self.model(x_tensor.to(self.device))\n",
    "        # Set it back to train mode\n",
    "        self.model.train()\n",
    "        # Detaches it, brings it to CPU and back to Numpy\n",
    "        return y_hat_tensor.detach().cpu().numpy()\n",
    "\n",
    "    def count_parameters(self):\n",
    "      return sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "\n",
    "    def plot_losses(self):\n",
    "        fig = plt.figure(figsize=(10, 4))\n",
    "        plt.plot(self.losses, label='Training Loss', c='b')\n",
    "        plt.plot(self.val_losses, label='Validation Loss', c='r')\n",
    "        plt.yscale('log')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "\n",
    "    @staticmethod\n",
    "    def _visualize_tensors(axs, x, y=None, yhat=None, layer_name='', title=None, class_names=None):\n",
    "        # The number of images is the number of subplots in a row\n",
    "        n_images = len(axs)\n",
    "        # Gets max and min values for scaling the grayscale\n",
    "        minv, maxv = np.min(x[:n_images]), np.max(x[:n_images])\n",
    "        # For each image\n",
    "        for j, image in enumerate(x[:n_images]):\n",
    "            ax = axs[j]\n",
    "            # Sets title, labels, and removes ticks\n",
    "            if title is not None:\n",
    "                ax.set_title(f'{title} #{j}', fontsize=12)\n",
    "            shp = np.atleast_2d(image).shape\n",
    "            ax.set_ylabel(\n",
    "                f'{layer_name}\\n{shp[0]}x{shp[1]}',\n",
    "                rotation=0, labelpad=40\n",
    "            )\n",
    "\n",
    "            if class_names is not None:\n",
    "                xlabel1 = '' if y is None else f'\\nReal: {class_names[y[j]]}'\n",
    "                xlabel2 = '' if yhat is None else f'\\nPred.: {class_names[yhat[j]]}'\n",
    "            else:\n",
    "                xlabel1 = '' if y is None else f'\\nLabel: {y[j]}'\n",
    "                xlabel2 = '' if yhat is None else f'\\nPredicted: {yhat[j]}'\n",
    "\n",
    "            xlabel = f'{xlabel1}{xlabel2}'\n",
    "            if len(xlabel):\n",
    "                ax.set_xlabel(xlabel, fontsize=12)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "\n",
    "            # Plots weight as an image\n",
    "            ax.imshow(\n",
    "                np.atleast_2d(image.squeeze()),\n",
    "                cmap='gray',\n",
    "                vmin=minv,\n",
    "                vmax=maxv\n",
    "            )\n",
    "        return\n",
    "\n",
    "    def visualize_filters(self, layer_name, **kwargs):\n",
    "        try:\n",
    "            # Gets the layer object from the model\n",
    "            layer = self.model\n",
    "            for name in layer_name.split('.'):\n",
    "                layer = getattr(layer, name)\n",
    "            # We are only looking at filters for 2D convolutions\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                # Takes the weight information\n",
    "                weights = layer.weight.data.cpu().numpy()\n",
    "                # weights -> (channels_out (filter), channels_in, H, W)\n",
    "                n_filters, n_channels, _, _ = weights.shape\n",
    "\n",
    "                # Builds a figure\n",
    "                size = (2 * n_channels + 2, 2 * n_filters)\n",
    "                fig, axes = plt.subplots(n_filters, n_channels,\n",
    "                                        figsize=size)\n",
    "                axes = np.atleast_2d(axes)\n",
    "                axes = axes.reshape(n_filters, n_channels)\n",
    "                # For each channel_out (filter)\n",
    "                for i in range(n_filters):\n",
    "                    Architecture._visualize_tensors(\n",
    "                        axes[i, :],\n",
    "                        weights[i],\n",
    "                        layer_name=f'Filter #{i}',\n",
    "                        title='Channel',\n",
    "                    )\n",
    "\n",
    "                for ax in axes.flat:\n",
    "                    ax.label_outer()\n",
    "\n",
    "                fig.tight_layout()\n",
    "                return fig\n",
    "        except AttributeError:\n",
    "            return\n",
    "\n",
    "    def attach_hooks(self, layers_to_hook, hook_fn=None):\n",
    "        # Clear any previous values\n",
    "        self.visualization = {}\n",
    "        # Creates the dictionary to map layer objects to their names\n",
    "        modules = list(self.model.named_modules())\n",
    "        layer_names = {layer: name for name, layer in modules[1:]}\n",
    "\n",
    "        if hook_fn is None:\n",
    "            # Hook function to be attached to the forward pass\n",
    "            def hook_fn(layer, inputs, outputs):\n",
    "                # Gets the layer name\n",
    "                name = layer_names[layer]\n",
    "                # Detaches outputs\n",
    "                values = outputs.detach().cpu().numpy()\n",
    "                # Since the hook function may be called multiple times\n",
    "                # for example, if we make predictions for multiple mini-batches\n",
    "                # it concatenates the results\n",
    "                if self.visualization[name] is None:\n",
    "                    self.visualization[name] = values\n",
    "                else:\n",
    "                    self.visualization[name] = np.concatenate([self.visualization[name], values])\n",
    "\n",
    "        for name, layer in modules:\n",
    "            # If the layer is in our list\n",
    "            if name in layers_to_hook:\n",
    "                # Initializes the corresponding key in the dictionary\n",
    "                self.visualization[name] = None\n",
    "                # Register the forward hook and keep the handle in another dict\n",
    "                self.handles[name] = layer.register_forward_hook(hook_fn)\n",
    "\n",
    "    def remove_hooks(self):\n",
    "        # Loops through all hooks and removes them\n",
    "        for handle in self.handles.values():\n",
    "            handle.remove()\n",
    "        # Clear the dict, as all hooks have been removed\n",
    "        self.handles = {}\n",
    "\n",
    "    def visualize_outputs(self, layers, n_images=10, y=None, yhat=None):\n",
    "        layers = filter(lambda l: l in self.visualization.keys(), layers)\n",
    "        layers = list(layers)\n",
    "        shapes = [self.visualization[layer].shape for layer in layers]\n",
    "        n_rows = [shape[1] if len(shape) == 4 else 1\n",
    "                  for shape in shapes]\n",
    "        total_rows = np.sum(n_rows)\n",
    "\n",
    "        fig, axes = plt.subplots(total_rows, n_images,\n",
    "                                figsize=(1.5*n_images, 1.5*total_rows))\n",
    "        axes = np.atleast_2d(axes).reshape(total_rows, n_images)\n",
    "\n",
    "        # Loops through the layers, one layer per row of subplots\n",
    "        row = 0\n",
    "        for i, layer in enumerate(layers):\n",
    "            start_row = row\n",
    "            # Takes the produced feature maps for that layer\n",
    "            output = self.visualization[layer]\n",
    "\n",
    "            is_vector = len(output.shape) == 2\n",
    "\n",
    "            for j in range(n_rows[i]):\n",
    "                Architecture._visualize_tensors(\n",
    "                    axes[row, :],\n",
    "                    output if is_vector else output[:, j].squeeze(),\n",
    "                    y,\n",
    "                    yhat,\n",
    "                    layer_name=layers[i] \\\n",
    "                              if is_vector \\\n",
    "                              else f'{layers[i]}\\nfil#{row-start_row}',\n",
    "                    title='Image' if (row == 0) else None,\n",
    "                    class_names=self.class_names\n",
    "                )\n",
    "                row += 1\n",
    "\n",
    "        for ax in axes.flat:\n",
    "            ax.label_outer()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "\n",
    "    def correct(self, x, y, threshold=.5):\n",
    "        self.model.eval()\n",
    "        yhat = self.model(x.to(self.device))\n",
    "        y = y.to(self.device)\n",
    "        self.model.train()\n",
    "\n",
    "        # We get the size of the batch and the number of classes\n",
    "        # (only 1, if it is binary)\n",
    "        n_samples, n_dims = yhat.shape\n",
    "        if n_dims > 1:\n",
    "            # In a multiclass classification, the biggest logit\n",
    "            # always wins, so we don't bother getting probabilities\n",
    "\n",
    "            # This is PyTorch's version of argmax,\n",
    "            # but it returns a tuple: (max value, index of max value)\n",
    "            _, predicted = torch.max(yhat, 1)\n",
    "        else:\n",
    "            n_dims += 1\n",
    "            # In binary classification, we NEED to check if the\n",
    "            # last layer is a sigmoid (and then it produces probs)\n",
    "            if isinstance(self.model, nn.Sequential) and \\\n",
    "              isinstance(self.model[-1], nn.Sigmoid):\n",
    "                predicted = (yhat > threshold).long()\n",
    "            # or something else (logits), which we need to convert\n",
    "            # using a sigmoid\n",
    "            else:\n",
    "                predicted = (F.sigmoid(yhat) > threshold).long()\n",
    "\n",
    "        # How many samples got classified correctly for each class\n",
    "        result = []\n",
    "        for c in range(n_dims):\n",
    "            n_class = (y == c).sum().item()\n",
    "            n_correct = (predicted[y == c] == c).sum().item()\n",
    "            result.append((n_correct, n_class))\n",
    "        return torch.tensor(result)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def loader_apply(loader, func, reduce='sum'):\n",
    "        results = [func(x, y) for i, (x, y) in enumerate(loader)]\n",
    "        results = torch.stack(results, axis=0)\n",
    "\n",
    "        if reduce == 'sum':\n",
    "            results = results.sum(axis=0)\n",
    "        elif reduce == 'mean':\n",
    "            results = results.float().mean(axis=0)\n",
    "\n",
    "        return results\n",
    "\n",
    "    @staticmethod\n",
    "    def statistics_per_channel(images, labels):\n",
    "        # NCHW\n",
    "        n_samples, n_channels, n_height, n_weight = images.size()\n",
    "        # Flatten HW into a single dimension\n",
    "        flatten_per_channel = images.reshape(n_samples, n_channels, -1)\n",
    "\n",
    "        # Computes statistics of each image per channel\n",
    "        # Average pixel value per channel\n",
    "        # (n_samples, n_channels)\n",
    "        means = flatten_per_channel.mean(axis=2)\n",
    "        # Standard deviation of pixel values per channel\n",
    "        # (n_samples, n_channels)\n",
    "        stds = flatten_per_channel.std(axis=2)\n",
    "\n",
    "        # Adds up statistics of all images in a mini-batch\n",
    "        # (1, n_channels)\n",
    "        sum_means = means.sum(axis=0)\n",
    "        sum_stds = stds.sum(axis=0)\n",
    "        # Makes a tensor of shape (1, n_channels)\n",
    "        # with the number of samples in the mini-batch\n",
    "        n_samples = torch.tensor([n_samples]*n_channels).float()\n",
    "\n",
    "        # Stack the three tensors on top of one another\n",
    "        # (3, n_channels)\n",
    "        return torch.stack([n_samples, sum_means, sum_stds], axis=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def make_normalizer(loader):\n",
    "        total_samples, total_means, total_stds = Architecture.loader_apply(loader, Architecture.statistics_per_channel)\n",
    "        norm_mean = total_means / total_samples\n",
    "        norm_std = total_stds / total_samples\n",
    "        return Normalize(mean=norm_mean, std=norm_std)\n",
    "\n",
    "    def set_optimizer(self, optimizer):\n",
    "        self.optimizer = optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJIlh3GhvU-T"
   },
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1752151348266,
     "user": {
      "displayName": "Angelo Marcelino",
      "userId": "12130434257979385717"
     },
     "user_tz": 180
    },
    "id": "VaooCxMXmSdc"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "def get_all_predictions(model, loader, device):\n",
    "    all_preds = torch.tensor([]).to(device)\n",
    "    all_labels = torch.tensor([]).to(device)\n",
    "    for images, labels in loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        preds = model(images)\n",
    "        all_preds = torch.cat(\n",
    "            (all_preds, preds)\n",
    "            , dim=0\n",
    "        )\n",
    "        all_labels = torch.cat(\n",
    "            (all_labels, labels)\n",
    "            , dim=0\n",
    "        )\n",
    "    return all_preds.cpu(), all_labels.cpu()\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = (cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] ) * 100.0\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    plt.grid(False)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize and cm[i, j] == 0:\n",
    "            continue\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aecieQzk0ena"
   },
   "source": [
    "# Generic Model Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1752151348267,
     "user": {
      "displayName": "Angelo Marcelino",
      "userId": "12130434257979385717"
     },
     "user_tz": 180
    },
    "id": "XPN1pkZ60jVN"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch.optim as optim  # Optimization algorithms\n",
    "\n",
    "class ModelFlow():\n",
    "\n",
    "    def __init__(self, model, train_loader, val_loader, model_name=\"model_instance\", load_checkpoint_path=None):\n",
    "        self.model = model\n",
    "        self.model_name = model_name\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "\n",
    "        self.results_dir = f\"results/{self.model_name}\"\n",
    "        os.makedirs(self.results_dir, exist_ok=True)\n",
    "        print(f\"Diretório de resultados: '{self.results_dir}'\")\n",
    "\n",
    "        torch.manual_seed(13)\n",
    "\n",
    "        self.multi_loss_fn = nn.CrossEntropyLoss(reduction='mean')\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "        self.class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "                           'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "        self.arch = Architecture(self.model,\n",
    "                                 self.multi_loss_fn,\n",
    "                                 self.optimizer,\n",
    "                                 class_names=self.class_names,\n",
    "                                 verbose=True)\n",
    "        self.arch.set_loaders(self.train_loader, self.val_loader)\n",
    "\n",
    "        if load_checkpoint_path:\n",
    "            print(f\"Carregando checkpoint de: {load_checkpoint_path}\")\n",
    "            self.arch.load_checkpoint(load_checkpoint_path)\n",
    "\n",
    "        print(f\"Modelo '{self.model_name}' - Parâmetros: {self.arch.count_parameters()}\")\n",
    "\n",
    "    def train(self, epochs=10):\n",
    "        print(\"Iniciando treinamento do modelo...\")\n",
    "        self.arch.train(epochs, verbose=True, verbose_mini_batch=False, batch_report=1)\n",
    "        print(\"Treinamento completo!\")\n",
    "\n",
    "    def evaluate(self):\n",
    "        print(\"\\nAvaliando o modelo e salvando métricas...\")\n",
    "        # Gráfico de Perdas\n",
    "        losses_fig = self.arch.plot_losses()\n",
    "        losses_fig.savefig(f\"{self.results_dir}/losses.png\")\n",
    "        plt.close(losses_fig)\n",
    "        print(f\"Gráfico de perdas salvo em '{self.results_dir}/losses.png'\")\n",
    "\n",
    "        # Matriz de Confusão\n",
    "        with torch.no_grad():\n",
    "            val_preds, val_labels = get_all_predictions(self.model, self.val_loader, self.arch.device)\n",
    "        cm = confusion_matrix(val_labels.numpy(), val_preds.argmax(dim=1).numpy())\n",
    "        fig_cm = plt.figure(figsize=(10,10))\n",
    "        plot_confusion_matrix(cm, self.class_names, normalize=True, title=f\"Matriz de Confusão - {self.model_name}\")\n",
    "        fig_cm.savefig(f\"{self.results_dir}/confusion_matrix.png\")\n",
    "        plt.close(fig_cm)\n",
    "        print(f\"Matriz de confusão salva em '{self.results_dir}/confusion_matrix.png'\")\n",
    "\n",
    "        # Relatório de Classificação\n",
    "        predicted_classes = val_preds.argmax(dim=1)\n",
    "        report = classification_report(val_labels.numpy(), predicted_classes.numpy(), target_names=self.class_names, digits=4, output_dict=True)\n",
    "        report_df = pd.DataFrame(report).transpose()\n",
    "        report_df.to_csv(f\"{self.results_dir}/classification_report.csv\")\n",
    "        print(\"Relatório de classificação:\")\n",
    "        print(report_df)\n",
    "        print(f\"Métricas salvas em '{self.results_dir}/'\")\n",
    "\n",
    "    def show_insides(self, filter_layers=[], layers_to_hook=[]):\n",
    "        # Visualização de filtros\n",
    "        for layer in filter_layers:\n",
    "            fig = self.arch.visualize_filters(layer)\n",
    "            if fig:\n",
    "                fig.savefig(f\"{self.results_dir}/filters_{layer}.png\")\n",
    "                plt.close(fig)\n",
    "                print(f\"Filtros da camada {layer} salvos em '{self.results_dir}/filters_{layer}.png'\")\n",
    "\n",
    "        # Pega um lote de imagens de validação\n",
    "        images, labels = next(iter(self.val_loader))\n",
    "        images_gpu = images.to(self.arch.device)\n",
    "\n",
    "        # Anexa os hooks\n",
    "        self.arch.attach_hooks(layers_to_hook)\n",
    "\n",
    "        # Passa os dados pelo modelo para capturar as saídas\n",
    "        with torch.no_grad():\n",
    "            output = self.arch.model(images_gpu)\n",
    "\n",
    "        predicted_classes = output.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "        # Adiciona as imagens originais para comparação no plot\n",
    "        self.arch.visualization['images'] = images.cpu().numpy()\n",
    "\n",
    "        # Visualiza as saídas capturadas\n",
    "        fig_outputs = self.arch.visualize_outputs(layers_to_hook, y=labels.numpy(), yhat=predicted_classes)\n",
    "        fig_outputs.savefig(f\"{self.results_dir}/intermediate_outputs.png\")\n",
    "        plt.close(fig_outputs)\n",
    "        print(f\"Saídas intermediárias salvas em '{self.results_dir}/intermediate_outputs.png'\")\n",
    "\n",
    "        # Remove os hooks\n",
    "        self.arch.remove_hooks()\n",
    "\n",
    "        # Mantém apenas os outputs textuais informando sobre o salvamento\n",
    "        print(f\"\\nFiguras de filtros e ativações salvas em '{self.results_dir}/'\")\n",
    "\n",
    "    def run(self, layers, epochs=10, skip_train=False):\n",
    "        if not skip_train:\n",
    "            self.train(epochs)\n",
    "\n",
    "        checkpoint_path = f\"{self.results_dir}/model_checkpoint.pth\"\n",
    "        self.arch.save_checkpoint(checkpoint_path)\n",
    "        print(f\"\\nCheckpoint final salvo em: {checkpoint_path}\")\n",
    "\n",
    "        self.evaluate()\n",
    "        self.show_insides(filter_layers=layers, layers_to_hook=layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XiWmKH4CZmdy"
   },
   "source": [
    "# Import Fashion MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z1M9kv0yfD-C"
   },
   "source": [
    "## Fashion-MNIST Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GW6vVNpgWKCW"
   },
   "source": [
    "This dataset is a collection of 70,000 grayscale images of clothing articles from the fashion retailer Zalando. It was created as a more challenging alternative to the original MNIST dataset of handwritten digits.\n",
    "\n",
    "Key Features:\n",
    "\n",
    "* **Images**: 28x28 pixels, grayscale.\n",
    "* **Training Set**: 60,000 examples.\n",
    "* **Test Set**: 10,000 examples.\n",
    "* **Classes**: 10 different categories of clothing.\n",
    "\n",
    "Classes:\n",
    "\n",
    "The labels are integers from 0 to 9, corresponding to the following classes:\n",
    "\n",
    "0. T-shirt/top\n",
    "1. Trouser\n",
    "2. Pullover\n",
    "3. Dress\n",
    "4. Coat\n",
    "5. Sandal\n",
    "6. Shirt\n",
    "7. Sneaker\n",
    "8. Bag\n",
    "9. Ankle boot\n",
    "\n",
    "It shares the same image size, training/testing split, and overall structure as the original MNIST dataset, making it a \"drop-in replacement\" for many machine learning and computer vision tasks.\n",
    "\n",
    "Original source https://github.com/zalandoresearch/fashion-mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-jrnp1uYjqE"
   },
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 78,
     "status": "ok",
     "timestamp": 1752151348346,
     "user": {
      "displayName": "Angelo Marcelino",
      "userId": "12130434257979385717"
     },
     "user_tz": 180
    },
    "id": "yrBloTRAZi7k"
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Download and load the training data\n",
    "trainset = torchvision.datasets.FashionMNIST(root='./data', train=True,\n",
    "                                        download=True,\n",
    "                                        transform=transforms.ToTensor())\n",
    "\n",
    "# Download and load the test data\n",
    "testset = torchvision.datasets.FashionMNIST(root='./data', train=False,\n",
    "                                       download=True,\n",
    "                                       transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 695,
     "status": "ok",
     "timestamp": 1752151349087,
     "user": {
      "displayName": "Angelo Marcelino",
      "userId": "12130434257979385717"
     },
     "user_tz": 180
    },
    "id": "qlytyzizdwBd",
    "outputId": "420af1ba-9419-442c-9533-088c71dfc063"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x600 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import ToPILImage\n",
    "\n",
    "def figure_fashion_by_class(dataset):\n",
    "    # These are the 10 class names for the dataset\n",
    "    class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "                   'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "    # --- Find one image for each class ---\n",
    "    images_by_class = {}\n",
    "    for image, label in dataset:\n",
    "        # If we haven't found an image for this class yet, store it\n",
    "        if label not in images_by_class:\n",
    "            images_by_class[label] = image\n",
    "        # Stop when we have one for each of the 10 classes\n",
    "        if len(images_by_class) == 10:\n",
    "            break\n",
    "\n",
    "    # --- Plot the images ---\n",
    "    # Create a figure with a 2x5 grid of subplots\n",
    "    fig, axs = plt.subplots(2, 5, figsize=(15, 6))\n",
    "\n",
    "    # Sort the images by class index (0-9) to keep the order consistent\n",
    "    sorted_labels = sorted(images_by_class.keys())\n",
    "\n",
    "    for i, ax in enumerate(axs.flat):\n",
    "        label_index = sorted_labels[i]\n",
    "        image_tensor = images_by_class[label_index]\n",
    "\n",
    "        # Convert the tensor image to a PIL Image for displaying\n",
    "        image_pil = ToPILImage()(image_tensor)\n",
    "\n",
    "        # Get the class name using the label index\n",
    "        title = class_names[label_index]\n",
    "\n",
    "        # Display the image\n",
    "        ax.imshow(image_pil, cmap='gray')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title(f\"{label_index}: {title}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the function to display one example of each class\n",
    "figure_fashion_by_class(trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0QDz6_gIfehS"
   },
   "source": [
    "# Data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6742,
     "status": "ok",
     "timestamp": 1752151355826,
     "user": {
      "displayName": "Angelo Marcelino",
      "userId": "12130434257979385717"
     },
     "user_tz": 180
    },
    "id": "Y3L5AogbfePT",
    "outputId": "f5e689e2-7fc2-44cf-a6ab-08761f6d321f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated Mean: 0.2860\n",
      "Calculated Std: 0.3530\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# --- Standardization and DataLoader Creation ---\n",
    "\n",
    "# 1. Load the training data to calculate the mean and standard deviation.\n",
    "# This ensures we only use training data statistics to avoid data leakage.\n",
    "temp_loader = torch.utils.data.DataLoader(trainset, batch_size=len(trainset))\n",
    "data = next(iter(temp_loader))\n",
    "mean = data[0].mean()\n",
    "std = data[0].std()\n",
    "print(f\"Calculated Mean: {mean:.4f}\")\n",
    "print(f\"Calculated Std: {std:.4f}\")\n",
    "\n",
    "# 2. Calculate normalization.\n",
    "composer = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "# 3. Reload the datasets using our new transformation pipeline.\n",
    "trainset_norm = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=composer)\n",
    "testset_norm = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=composer)\n",
    "\n",
    "# 4. Create the final DataLoaders for training and validation.\n",
    "train_loader = torch.utils.data.DataLoader(trainset_norm, batch_size=16, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(testset_norm, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cw_2zOkdi7hS"
   },
   "source": [
    "# Model Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1752151355830,
     "user": {
      "displayName": "Angelo Marcelino",
      "userId": "12130434257979385717"
     },
     "user_tz": 180
    },
    "id": "kkyX4VHxi6tz"
   },
   "outputs": [],
   "source": [
    "class CNN2(nn.Module):\n",
    "    def __init__(self, n_feature, in_channels=1, num_classes=10, p=0.3):\n",
    "        super(CNN2, self).__init__()\n",
    "        self.n_feature = n_feature\n",
    "        self.p = p\n",
    "        # Creates the convolution layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels,\n",
    "                               out_channels=n_feature,\n",
    "                               kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(in_channels=n_feature,\n",
    "                               out_channels=n_feature,\n",
    "                               kernel_size=3)\n",
    "        # Creates the linear layers\n",
    "        # Where do this 5 * 5 come from?! Check it below\n",
    "        self.fc1 = nn.Linear(n_feature * 5 * 5, 50)\n",
    "        self.fc2 = nn.Linear(50, num_classes)\n",
    "        # Creates dropout layers\n",
    "        self.drop = nn.Dropout(self.p)\n",
    "\n",
    "    def featurizer(self, x):\n",
    "        # Featurizer\n",
    "        # First convolutional block\n",
    "        # 3@28x28 -> n_feature@26x26 -> n_feature@13x13\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "        # Second convolutional block\n",
    "        # n_feature * @13x13 -> n_feature@11x11 -> n_feature@5x5\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "        # Input dimension (n_feature@5x5)\n",
    "        # Output dimension (n_feature * 5 * 5)\n",
    "        x = nn.Flatten()(x)\n",
    "        return x\n",
    "\n",
    "    def classifier(self, x):\n",
    "        # Classifier\n",
    "        # Hidden Layer\n",
    "        # Input dimension (n_feature * 5 * 5)\n",
    "        # Output dimension (50)\n",
    "        if self.p > 0:\n",
    "            x = self.drop(x)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        # Output Layer\n",
    "        # Input dimension (50)\n",
    "        # Output dimension (3)\n",
    "        if self.p > 0:\n",
    "            x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.featurizer(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l5YuSpDH3Scb"
   },
   "source": [
    "# Model Base Flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 248762,
     "status": "ok",
     "timestamp": 1752151604594,
     "user": {
      "displayName": "Angelo Marcelino",
      "userId": "12130434257979385717"
     },
     "user_tz": 180
    },
    "id": "TdtlDUCP4x--",
    "outputId": "888e063b-63fc-4fc2-9156-62bb66d95397"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diretório de resultados: 'results/cnn2_7'\n",
      "Model sent to cuda\n",
      "Architecture created\n",
      "Loaders set\n",
      "Train dataset size: 60000\n",
      "Train batch size: 16\n",
      "Modelo 'cnn2_7' - Parâmetros: 9828\n",
      "Iniciando treinamento do modelo...\n",
      "Random seed set to 42\n",
      "Starting training...\n",
      "Epoch 1/10 | Train loss: 0.88050974 | Val. loss: 0.58427529 | Time: 24.20s | ETA: 3m 37s\n",
      "Epoch 2/10 | Train loss: 0.63816708 | Val. loss: 0.52531796 | Time: 24.21s | ETA: 3m 13s\n",
      "Epoch 3/10 | Train loss: 0.58911124 | Val. loss: 0.49274784 | Time: 24.68s | ETA: 2m 50s\n",
      "Epoch 4/10 | Train loss: 0.55884520 | Val. loss: 0.46563246 | Time: 24.45s | ETA: 2m 26s\n",
      "Epoch 5/10 | Train loss: 0.54338683 | Val. loss: 0.45305204 | Time: 24.43s | ETA: 2m 1s\n",
      "Epoch 6/10 | Train loss: 0.52554869 | Val. loss: 0.44374681 | Time: 24.15s | ETA: 1m 37s\n",
      "Epoch 7/10 | Train loss: 0.51304412 | Val. loss: 0.42839735 | Time: 23.85s | ETA: 1m 12s\n",
      "Epoch 8/10 | Train loss: 0.50159796 | Val. loss: 0.41832404 | Time: 23.80s | ETA: 0m 48s\n",
      "Epoch 9/10 | Train loss: 0.49407938 | Val. loss: 0.40700379 | Time: 23.82s | ETA: 0m 24s\n",
      "Epoch 10/10 | Train loss: 0.48733721 | Val. loss: 0.40641880 | Time: 24.01s | ETA: 0m 0s\n",
      "Training completed in 241.59 seconds.\n",
      "Mean time per epoch: 24.16 seconds.\n",
      "Treinamento completo!\n",
      "Checkpoint saved to results/cnn2_7/model_checkpoint.pth\n",
      "\n",
      "Checkpoint final salvo em: results/cnn2_7/model_checkpoint.pth\n",
      "\n",
      "Avaliando o modelo e salvando métricas...\n",
      "Gráfico de perdas salvo em 'results/cnn2_7/losses.png'\n",
      "Normalized confusion matrix\n",
      "Matriz de confusão salva em 'results/cnn2_7/confusion_matrix.png'\n",
      "Relatório de classificação:\n",
      "              precision  recall  f1-score     support\n",
      "T-shirt/top    0.821063  0.8030  0.811931   1000.0000\n",
      "Trouser        0.982724  0.9670  0.974798   1000.0000\n",
      "Pullover       0.759221  0.7410  0.750000   1000.0000\n",
      "Dress          0.834443  0.8770  0.855193   1000.0000\n",
      "Coat           0.687122  0.7950  0.737135   1000.0000\n",
      "Sandal         0.957384  0.9660  0.961672   1000.0000\n",
      "Shirt          0.617827  0.5060  0.556350   1000.0000\n",
      "Sneaker        0.945720  0.9060  0.925434   1000.0000\n",
      "Bag            0.941520  0.9660  0.953603   1000.0000\n",
      "Ankle boot     0.929942  0.9690  0.949070   1000.0000\n",
      "accuracy       0.849600  0.8496  0.849600      0.8496\n",
      "macro avg      0.847697  0.8496  0.847519  10000.0000\n",
      "weighted avg   0.847697  0.8496  0.847519  10000.0000\n",
      "Métricas salvas em 'results/cnn2_7/'\n",
      "Filtros da camada conv1 salvos em 'results/cnn2_7/filters_conv1.png'\n",
      "Filtros da camada conv2 salvos em 'results/cnn2_7/filters_conv2.png'\n",
      "Saídas intermediárias salvas em 'results/cnn2_7/intermediate_outputs.png'\n",
      "\n",
      "Figuras de filtros e ativações salvas em 'results/cnn2_7/'\n"
     ]
    }
   ],
   "source": [
    "model_cnn2_7 = CNN2(n_feature=7, p=0.3, in_channels=1, num_classes=10)\n",
    "\n",
    "mflow_7 = ModelFlow(model=model_cnn2_7, train_loader=train_loader, val_loader=val_loader, model_name='cnn2_7')\n",
    "mflow_7.run(layers=['conv1', 'conv2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 235273,
     "status": "ok",
     "timestamp": 1752151839869,
     "user": {
      "displayName": "Angelo Marcelino",
      "userId": "12130434257979385717"
     },
     "user_tz": 180
    },
    "id": "OxCnTt_B3Vgr",
    "outputId": "c1f7984c-0a0a-49de-d49e-43adbb25d399"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diretório de resultados: 'results/cnn2_5'\n",
      "Model sent to cuda\n",
      "Architecture created\n",
      "Loaders set\n",
      "Train dataset size: 60000\n",
      "Train batch size: 16\n",
      "Modelo 'cnn2_5' - Parâmetros: 7090\n",
      "Iniciando treinamento do modelo...\n",
      "Random seed set to 42\n",
      "Starting training...\n",
      "Epoch 1/10 | Train loss: 0.90339681 | Val. loss: 0.57499800 | Time: 22.78s | ETA: 3m 24s\n",
      "Epoch 2/10 | Train loss: 0.64612754 | Val. loss: 0.51932986 | Time: 23.07s | ETA: 3m 3s\n",
      "Epoch 3/10 | Train loss: 0.59988326 | Val. loss: 0.49184304 | Time: 22.86s | ETA: 2m 40s\n",
      "Epoch 4/10 | Train loss: 0.57369471 | Val. loss: 0.47410615 | Time: 23.06s | ETA: 2m 17s\n",
      "Epoch 5/10 | Train loss: 0.55264102 | Val. loss: 0.46165605 | Time: 23.39s | ETA: 1m 55s\n",
      "Epoch 6/10 | Train loss: 0.54753817 | Val. loss: 0.45264621 | Time: 23.19s | ETA: 1m 32s\n",
      "Epoch 7/10 | Train loss: 0.53618633 | Val. loss: 0.44262220 | Time: 22.65s | ETA: 1m 8s\n",
      "Epoch 8/10 | Train loss: 0.52653868 | Val. loss: 0.44237142 | Time: 22.79s | ETA: 0m 45s\n",
      "Epoch 9/10 | Train loss: 0.51749361 | Val. loss: 0.43107122 | Time: 22.71s | ETA: 0m 22s\n",
      "Epoch 10/10 | Train loss: 0.51500918 | Val. loss: 0.43788812 | Time: 22.67s | ETA: 0m 0s\n",
      "Training completed in 229.16 seconds.\n",
      "Mean time per epoch: 22.92 seconds.\n",
      "Treinamento completo!\n",
      "Checkpoint saved to results/cnn2_5/model_checkpoint.pth\n",
      "\n",
      "Checkpoint final salvo em: results/cnn2_5/model_checkpoint.pth\n",
      "\n",
      "Avaliando o modelo e salvando métricas...\n",
      "Gráfico de perdas salvo em 'results/cnn2_5/losses.png'\n",
      "Normalized confusion matrix\n",
      "Matriz de confusão salva em 'results/cnn2_5/confusion_matrix.png'\n",
      "Relatório de classificação:\n",
      "              precision  recall  f1-score     support\n",
      "T-shirt/top    0.795833  0.7640  0.779592   1000.0000\n",
      "Trouser        0.982671  0.9640  0.973246   1000.0000\n",
      "Pullover       0.704545  0.6820  0.693089   1000.0000\n",
      "Dress          0.819277  0.8840  0.850409   1000.0000\n",
      "Coat           0.648936  0.7930  0.713771   1000.0000\n",
      "Sandal         0.960883  0.9580  0.959439   1000.0000\n",
      "Shirt          0.610442  0.4560  0.522038   1000.0000\n",
      "Sneaker        0.944974  0.8930  0.918252   1000.0000\n",
      "Bag            0.928846  0.9660  0.947059   1000.0000\n",
      "Ankle boot     0.912347  0.9680  0.939350   1000.0000\n",
      "accuracy       0.832800  0.8328  0.832800      0.8328\n",
      "macro avg      0.830875  0.8328  0.829624  10000.0000\n",
      "weighted avg   0.830875  0.8328  0.829624  10000.0000\n",
      "Métricas salvas em 'results/cnn2_5/'\n",
      "Filtros da camada conv1 salvos em 'results/cnn2_5/filters_conv1.png'\n",
      "Filtros da camada conv2 salvos em 'results/cnn2_5/filters_conv2.png'\n",
      "Saídas intermediárias salvas em 'results/cnn2_5/intermediate_outputs.png'\n",
      "\n",
      "Figuras de filtros e ativações salvas em 'results/cnn2_5/'\n"
     ]
    }
   ],
   "source": [
    "model_cnn2 = CNN2(n_feature=5, p=0.3, in_channels=1, num_classes=10)\n",
    "\n",
    "mflow = ModelFlow(model=model_cnn2, train_loader=train_loader, val_loader=val_loader, model_name='cnn2_5')\n",
    "mflow.run(layers=['conv1', 'conv2'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 233793,
     "status": "ok",
     "timestamp": 1752152073646,
     "user": {
      "displayName": "Angelo Marcelino",
      "userId": "12130434257979385717"
     },
     "user_tz": 180
    },
    "id": "7shPFKaQ5QQK",
    "outputId": "5f205ea1-9823-4b06-9a49-740fa8a898ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diretório de resultados: 'results/cnn2_3'\n",
      "Model sent to cuda\n",
      "Architecture created\n",
      "Loaders set\n",
      "Train dataset size: 60000\n",
      "Train batch size: 16\n",
      "Modelo 'cnn2_3' - Parâmetros: 4424\n",
      "Iniciando treinamento do modelo...\n",
      "Random seed set to 42\n",
      "Starting training...\n",
      "Epoch 1/10 | Train loss: 1.07989070 | Val. loss: 0.68932869 | Time: 22.45s | ETA: 3m 22s\n",
      "Epoch 2/10 | Train loss: 0.80857326 | Val. loss: 0.63314324 | Time: 22.69s | ETA: 3m 0s\n",
      "Epoch 3/10 | Train loss: 0.76727806 | Val. loss: 0.61064616 | Time: 22.97s | ETA: 2m 38s\n",
      "Epoch 4/10 | Train loss: 0.73847971 | Val. loss: 0.59500342 | Time: 25.35s | ETA: 2m 20s\n",
      "Epoch 5/10 | Train loss: 0.72014742 | Val. loss: 0.58285936 | Time: 22.85s | ETA: 1m 56s\n",
      "Epoch 6/10 | Train loss: 0.70669165 | Val. loss: 0.56912378 | Time: 22.72s | ETA: 1m 32s\n",
      "Epoch 7/10 | Train loss: 0.69934373 | Val. loss: 0.56097420 | Time: 22.69s | ETA: 1m 9s\n",
      "Epoch 8/10 | Train loss: 0.68945315 | Val. loss: 0.55640864 | Time: 22.21s | ETA: 0m 45s\n",
      "Epoch 9/10 | Train loss: 0.66198533 | Val. loss: 0.53196683 | Time: 22.84s | ETA: 0m 22s\n",
      "Epoch 10/10 | Train loss: 0.64320046 | Val. loss: 0.52157647 | Time: 22.67s | ETA: 0m 0s\n",
      "Training completed in 229.44 seconds.\n",
      "Mean time per epoch: 22.94 seconds.\n",
      "Treinamento completo!\n",
      "Checkpoint saved to results/cnn2_3/model_checkpoint.pth\n",
      "\n",
      "Checkpoint final salvo em: results/cnn2_3/model_checkpoint.pth\n",
      "\n",
      "Avaliando o modelo e salvando métricas...\n",
      "Gráfico de perdas salvo em 'results/cnn2_3/losses.png'\n",
      "Normalized confusion matrix\n",
      "Matriz de confusão salva em 'results/cnn2_3/confusion_matrix.png'\n",
      "Relatório de classificação:\n",
      "              precision  recall  f1-score     support\n",
      "T-shirt/top    0.736347  0.7820  0.758487   1000.0000\n",
      "Trouser        0.974280  0.9470  0.960446   1000.0000\n",
      "Pullover       0.722045  0.6780  0.699330   1000.0000\n",
      "Dress          0.750220  0.8530  0.798315   1000.0000\n",
      "Coat           0.623478  0.7170  0.666977   1000.0000\n",
      "Sandal         0.933067  0.9340  0.933533   1000.0000\n",
      "Shirt          0.536415  0.3830  0.446908   1000.0000\n",
      "Sneaker        0.931111  0.8380  0.882105   1000.0000\n",
      "Bag            0.929773  0.9400  0.934858   1000.0000\n",
      "Ankle boot     0.869838  0.9690  0.916746   1000.0000\n",
      "accuracy       0.804100  0.8041  0.804100      0.8041\n",
      "macro avg      0.800657  0.8041  0.799770  10000.0000\n",
      "weighted avg   0.800657  0.8041  0.799770  10000.0000\n",
      "Métricas salvas em 'results/cnn2_3/'\n",
      "Filtros da camada conv1 salvos em 'results/cnn2_3/filters_conv1.png'\n",
      "Filtros da camada conv2 salvos em 'results/cnn2_3/filters_conv2.png'\n",
      "Saídas intermediárias salvas em 'results/cnn2_3/intermediate_outputs.png'\n",
      "\n",
      "Figuras de filtros e ativações salvas em 'results/cnn2_3/'\n"
     ]
    }
   ],
   "source": [
    "model_cnn2_3= CNN2(n_feature=3, p=0.3, in_channels=1, num_classes=10)\n",
    "\n",
    "mflow_3 = ModelFlow(model=model_cnn2_3, train_loader=train_loader, val_loader=val_loader, model_name='cnn2_3')\n",
    "mflow_3.run(layers=['conv1', 'conv2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TVELNv_8-apz"
   },
   "source": [
    "# Generic CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1752152073647,
     "user": {
      "displayName": "Angelo Marcelino",
      "userId": "12130434257979385717"
     },
     "user_tz": 180
    },
    "id": "tkUT2F8I-cNI"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class GerenicCNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 num_classes,\n",
    "                 input_size=28,\n",
    "                 amnt_cov_layers=2,\n",
    "                 cov_kernel_sizes=None,\n",
    "                 cov_features=None,\n",
    "                 max_pool_sizes=None,\n",
    "                 amnt_fc_layers=2,\n",
    "                 fc_sizes=None,\n",
    "                 p=0.3):\n",
    "        super(GerenicCNN, self).__init__()\n",
    "\n",
    "        # Define defaults se não fornecidos\n",
    "        if cov_kernel_sizes is None:\n",
    "            cov_kernel_sizes = [3] * amnt_cov_layers\n",
    "        if cov_features is None:\n",
    "            cov_features = [16] * amnt_cov_layers\n",
    "        if max_pool_sizes is None:\n",
    "            max_pool_sizes = [2] * amnt_cov_layers\n",
    "        if fc_sizes is None:\n",
    "            fc_sizes = [50] * (amnt_fc_layers - 1)  # só intermediários\n",
    "\n",
    "        # Validação\n",
    "        self._validate_parameters(amnt_cov_layers,\n",
    "                                 cov_kernel_sizes,\n",
    "                                 cov_features,\n",
    "                                 max_pool_sizes,\n",
    "                                 amnt_fc_layers,\n",
    "                                 fc_sizes)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.num_classes = num_classes\n",
    "        self.input_size = input_size\n",
    "        self.amnt_cov_layers = amnt_cov_layers\n",
    "        self.cov_kernel_sizes = cov_kernel_sizes\n",
    "        self.cov_features = cov_features\n",
    "        self.max_pool_sizes = max_pool_sizes\n",
    "        self.amnt_fc_layers = amnt_fc_layers\n",
    "        self.fc_sizes = fc_sizes\n",
    "        self.p = p\n",
    "\n",
    "        # Criação das camadas convolucionais e maxPool\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        self.max_pool_layers = nn.ModuleList()\n",
    "\n",
    "        current_channels = in_channels\n",
    "        current_size = input_size\n",
    "        for i in range(amnt_cov_layers):\n",
    "\n",
    "            if current_size <= 1:\n",
    "                msg = f'Não será possível criar a camada convolucional {i}, '\n",
    "                msg += \"pois não será possível aplicar MaxPool de pelo menos 2.\"\n",
    "                raise ValueError(msg)\n",
    "\n",
    "            self.conv_layers.append(\n",
    "                nn.Conv2d(\n",
    "                    in_channels=current_channels,\n",
    "                    out_channels=cov_features[i],\n",
    "                    kernel_size=cov_kernel_sizes[i]\n",
    "                )\n",
    "            )\n",
    "\n",
    "            self.max_pool_layers.append(\n",
    "                nn.MaxPool2d(\n",
    "                    kernel_size=max_pool_sizes[i]\n",
    "                )\n",
    "            )\n",
    "            conv_out_size, out_size = self._compute_next_dims(current_size, cov_kernel_sizes[i], max_pool_sizes[i])\n",
    "\n",
    "            print(f'conv{i}', end=' - ')\n",
    "            print(f'in_size: {current_size}x{current_size}', end='\\n\\t')\n",
    "            print(f'in_channels: {current_channels}', end='\\n\\t')\n",
    "            print(f'out_channels: {cov_features[i]}', end='\\n\\t')\n",
    "            print(f'kernel_size: {cov_kernel_sizes[i]}', end='\\n\\t')\n",
    "            print(f'conv_out_size: {conv_out_size}x{conv_out_size}', end='\\n\\t')\n",
    "            print(f'maxpool_kernel_size: {max_pool_sizes[i]}', end='\\n\\t')\n",
    "            print(f'out_size (after max pool): {out_size}x{out_size}')\n",
    "\n",
    "\n",
    "            current_channels = cov_features[i]\n",
    "            current_size = out_size\n",
    "\n",
    "            if current_size < 1:\n",
    "                msg = f'Erro! Saída da camada convolucional {i} calculada com dimensão 0'\n",
    "                raise ValueError(msg)\n",
    "\n",
    "\n",
    "        # Calcula flattened_dim para o fc\n",
    "        # Criar um tensor de exemplo para calcular a dimensão flattened\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.randn(1, in_channels, input_size, input_size)\n",
    "            dummy_output = self.featurizer(dummy_input)\n",
    "            self.flattened_dim = dummy_output.numel() // dummy_output.shape[0]\n",
    "\n",
    "\n",
    "        # Criação das camadas fully connected\n",
    "        self.fc_layers = nn.ModuleList()\n",
    "\n",
    "        full_fc_sizes = [self.flattened_dim] + self.fc_sizes + [num_classes]\n",
    "\n",
    "        for i in range(amnt_fc_layers):\n",
    "            print(f'fc{i} - in: {full_fc_sizes[i]} - out: {full_fc_sizes[i + 1]}')\n",
    "            self.fc_layers.append(\n",
    "                nn.Linear(\n",
    "                    in_features=full_fc_sizes[i],\n",
    "                    out_features=full_fc_sizes[i + 1]\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.drop = nn.Dropout(self.p)\n",
    "\n",
    "    def _validate_parameters(self, amnt_cov_layers, cov_kernel_sizes, cov_features, max_pool_sizes,\n",
    "                        amnt_fc_layers, fc_sizes):\n",
    "        assert len(cov_kernel_sizes) == amnt_cov_layers, \\\n",
    "            f\"'cov_kernel_sizes' deve ter {amnt_cov_layers} elementos, mas tem {len(cov_kernel_sizes)}.\"\n",
    "        assert len(cov_features) == amnt_cov_layers, \\\n",
    "            f\"'cov_features' deve ter {amnt_cov_layers} elementos, mas tem {len(cov_features)}.\"\n",
    "        assert len(max_pool_sizes) == amnt_cov_layers, \\\n",
    "            f\"'max_pool_sizes' deve ter {amnt_cov_layers} elementos, mas tem {len(max_pool_sizes)}.\"\n",
    "        assert len(fc_sizes) == (amnt_fc_layers - 1), \\\n",
    "            f\"'fc_sizes' deve ter {amnt_fc_layers - 1} elementos, mas tem {len(fc_sizes)}.\"\n",
    "\n",
    "\n",
    "    def _compute_next_dims(self, current_dim, kernel_size, max_pool_size):\n",
    "\n",
    "        # Cálculo é [(w - k + 2p) / s] + 1\n",
    "        # Por padrão, o padding é 0 e o stride é 1\n",
    "        # Então o cálculo fica = [(w - k) / 1] + 1\n",
    "        conv_dim = (current_dim - kernel_size) + 1\n",
    "\n",
    "\n",
    "        # Por padrão, o padding é 0 e o stride é o kernel size\n",
    "        # Então o cálculo fica = [(w - k) / k] + 1\n",
    "        # = [(w / k) - 1] + 1\n",
    "        # = w / k\n",
    "        after_pool_dim = conv_dim / max_pool_size\n",
    "\n",
    "        # Arredonda para baixo\n",
    "        after_pool_dim = math.floor(after_pool_dim)\n",
    "        return conv_dim, after_pool_dim\n",
    "\n",
    "    def featurizer(self, x):\n",
    "        # for i, conv_layer in enumerate(self.conv_layers):\n",
    "        #     x = conv_layer(x)\n",
    "        #     x = F.relu(x)\n",
    "        #     x = F.max_pool2d(x, kernel_size=self.max_pool_sizes[i])\n",
    "        # x = torch.flatten(x, 1)  # flatten exceto a batch dimension\n",
    "\n",
    "        for conv_layer, max_pool_layer in zip(self.conv_layers, self.max_pool_layers):\n",
    "            x = conv_layer(x)\n",
    "            x = F.relu(x)\n",
    "            x = max_pool_layer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "    def classifier(self, x):\n",
    "        x = torch.flatten(x, 1)\n",
    "        for i, fc_layer in enumerate(self.fc_layers):\n",
    "            if self.p > 0 and i < len(self.fc_layers) - 1: # Apply dropout only before non-output layers\n",
    "                x = self.drop(x)\n",
    "            x = fc_layer(x)\n",
    "            # Aplica ReLU em todas as camadas, exceto a última\n",
    "            if i < len(self.fc_layers) - 1:\n",
    "                x = F.relu(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.featurizer(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1752152270900,
     "user": {
      "displayName": "Angelo Marcelino",
      "userId": "12130434257979385717"
     },
     "user_tz": 180
    },
    "id": "5a0fnk_iO3j6"
   },
   "outputs": [],
   "source": [
    "example_dicts = [\n",
    "\n",
    "    #  CNN2 equivalente\n",
    "    {\n",
    "        \"in_channels\": 1,\n",
    "        \"num_classes\": 10,\n",
    "        \"input_size\": 28,\n",
    "        \"amnt_cov_layers\": 2,\n",
    "        \"cov_kernel_sizes\": [3, 3],\n",
    "        \"cov_features\": [5, 5],\n",
    "        \"max_pool_sizes\": [2, 2],\n",
    "        \"amnt_fc_layers\": 2,\n",
    "        \"fc_sizes\": [50],\n",
    "        \"p\": 0.3\n",
    "    },\n",
    "\n",
    "    # Rede mais profunda\n",
    "    {\n",
    "        \"in_channels\": 1,\n",
    "        \"num_classes\": 10,\n",
    "        \"input_size\": 28,\n",
    "        \"amnt_cov_layers\": 3,\n",
    "        \"cov_kernel_sizes\": [3, 3, 3],\n",
    "        \"cov_features\": [8, 16, 32],\n",
    "        \"max_pool_sizes\": [2, 2, 2],\n",
    "        \"amnt_fc_layers\": 3,\n",
    "        \"fc_sizes\": [100, 50],\n",
    "        \"p\": 0.3\n",
    "    },\n",
    "\n",
    "    # Rede Pequena\n",
    "    {\n",
    "        \"in_channels\": 1,\n",
    "        \"num_classes\": 10,\n",
    "        \"input_size\": 28,\n",
    "        \"amnt_cov_layers\": 1,\n",
    "        \"cov_kernel_sizes\": [5],\n",
    "        \"cov_features\": [4],\n",
    "        \"max_pool_sizes\": [2],\n",
    "        \"amnt_fc_layers\": 2,\n",
    "        \"fc_sizes\": [20],\n",
    "        \"p\": 0.3\n",
    "    },\n",
    "\n",
    "    # Rede Muitos FC\n",
    "    {\n",
    "        \"in_channels\": 1,\n",
    "        \"num_classes\": 10,\n",
    "        \"input_size\": 28,\n",
    "        \"amnt_cov_layers\": 2,\n",
    "        \"cov_kernel_sizes\": [3, 3],\n",
    "        \"cov_features\": [5, 5],\n",
    "        \"max_pool_sizes\": [2, 2],\n",
    "        \"amnt_fc_layers\": 10,\n",
    "        \"fc_sizes\": [int(1024 + i * (25 - 1024) / (9 - 1)) for i in range(9)],\n",
    "        \"p\": 0.3\n",
    "    },\n",
    "\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vr_EzTAEQJrC"
   },
   "source": [
    "# Other Models Flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 233560,
     "status": "ok",
     "timestamp": 1752152505916,
     "user": {
      "displayName": "Angelo Marcelino",
      "userId": "12130434257979385717"
     },
     "user_tz": 180
    },
    "id": "dIq8Qy1LO4Ds",
    "outputId": "478c96c0-4ec7-4652-c581-f8a90a1d94e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv0 - in_size: 28x28\n",
      "\tin_channels: 1\n",
      "\tout_channels: 5\n",
      "\tkernel_size: 3\n",
      "\tconv_out_size: 26x26\n",
      "\tmaxpool_kernel_size: 2\n",
      "\tout_size (after max pool): 13x13\n",
      "conv1 - in_size: 13x13\n",
      "\tin_channels: 5\n",
      "\tout_channels: 5\n",
      "\tkernel_size: 3\n",
      "\tconv_out_size: 11x11\n",
      "\tmaxpool_kernel_size: 2\n",
      "\tout_size (after max pool): 5x5\n",
      "fc0 - in: 125 - out: 50\n",
      "fc1 - in: 50 - out: 10\n",
      "Diretório de resultados: 'results/generic_0'\n",
      "Model sent to cuda\n",
      "Architecture created\n",
      "Loaders set\n",
      "Train dataset size: 60000\n",
      "Train batch size: 16\n",
      "Modelo 'generic_0' - Parâmetros: 7090\n",
      "Iniciando treinamento do modelo...\n",
      "Random seed set to 42\n",
      "Starting training...\n",
      "Epoch 1/10 | Train loss: 0.79236976 | Val. loss: 0.56087085 | Time: 24.74s | ETA: 3m 42s\n",
      "Epoch 2/10 | Train loss: 0.59245773 | Val. loss: 0.51068630 | Time: 22.64s | ETA: 3m 9s\n",
      "Epoch 3/10 | Train loss: 0.54753794 | Val. loss: 0.48120186 | Time: 22.53s | ETA: 2m 43s\n",
      "Epoch 4/10 | Train loss: 0.51667132 | Val. loss: 0.45644677 | Time: 23.03s | ETA: 2m 19s\n",
      "Epoch 5/10 | Train loss: 0.49907253 | Val. loss: 0.44064197 | Time: 22.50s | ETA: 1m 55s\n",
      "Epoch 6/10 | Train loss: 0.48157483 | Val. loss: 0.43240283 | Time: 22.47s | ETA: 1m 31s\n",
      "Epoch 7/10 | Train loss: 0.47179255 | Val. loss: 0.42051485 | Time: 22.28s | ETA: 1m 8s\n",
      "Epoch 8/10 | Train loss: 0.45802714 | Val. loss: 0.41047383 | Time: 22.39s | ETA: 0m 45s\n",
      "Epoch 9/10 | Train loss: 0.45072300 | Val. loss: 0.39831726 | Time: 22.71s | ETA: 0m 22s\n",
      "Epoch 10/10 | Train loss: 0.43697770 | Val. loss: 0.39774325 | Time: 21.86s | ETA: 0m 0s\n",
      "Training completed in 227.16 seconds.\n",
      "Mean time per epoch: 22.72 seconds.\n",
      "Treinamento completo!\n",
      "Checkpoint saved to results/generic_0/model_checkpoint.pth\n",
      "\n",
      "Checkpoint final salvo em: results/generic_0/model_checkpoint.pth\n",
      "\n",
      "Avaliando o modelo e salvando métricas...\n",
      "Gráfico de perdas salvo em 'results/generic_0/losses.png'\n",
      "Normalized confusion matrix\n",
      "Matriz de confusão salva em 'results/generic_0/confusion_matrix.png'\n",
      "Relatório de classificação:\n",
      "              precision  recall  f1-score     support\n",
      "T-shirt/top    0.829880  0.7610  0.793949   1000.0000\n",
      "Trouser        0.989744  0.9650  0.977215   1000.0000\n",
      "Pullover       0.750973  0.7720  0.761341   1000.0000\n",
      "Dress          0.861328  0.8820  0.871542   1000.0000\n",
      "Coat           0.681345  0.8510  0.756781   1000.0000\n",
      "Sandal         0.956436  0.9660  0.961194   1000.0000\n",
      "Shirt          0.643959  0.5010  0.563555   1000.0000\n",
      "Sneaker        0.952077  0.8940  0.922125   1000.0000\n",
      "Bag            0.944227  0.9650  0.954500   1000.0000\n",
      "Ankle boot     0.916824  0.9700  0.942663   1000.0000\n",
      "accuracy       0.852700  0.8527  0.852700      0.8527\n",
      "macro avg      0.852679  0.8527  0.850486  10000.0000\n",
      "weighted avg   0.852679  0.8527  0.850486  10000.0000\n",
      "Métricas salvas em 'results/generic_0/'\n",
      "Filtros da camada conv_layers.0 salvos em 'results/generic_0/filters_conv_layers.0.png'\n",
      "Filtros da camada conv_layers.1 salvos em 'results/generic_0/filters_conv_layers.1.png'\n",
      "Saídas intermediárias salvas em 'results/generic_0/intermediate_outputs.png'\n",
      "\n",
      "Figuras de filtros e ativações salvas em 'results/generic_0/'\n"
     ]
    }
   ],
   "source": [
    "model_generic_0 = GerenicCNN(**example_dicts[0])\n",
    "\n",
    "mflow_generic_0 = ModelFlow(\n",
    "    model=model_generic_0,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    model_name='generic_0'\n",
    ")\n",
    "\n",
    "mflow_generic_0.run(\n",
    "    layers=[f'conv_layers.{i}' for i in range(model_generic_0.amnt_cov_layers)]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 283716,
     "status": "ok",
     "timestamp": 1752152789633,
     "user": {
      "displayName": "Angelo Marcelino",
      "userId": "12130434257979385717"
     },
     "user_tz": 180
    },
    "id": "VC-1PxnmQ0sg",
    "outputId": "022addd1-e236-48c0-d714-011a4a4891e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv0 - in_size: 28x28\n",
      "\tin_channels: 1\n",
      "\tout_channels: 8\n",
      "\tkernel_size: 3\n",
      "\tconv_out_size: 26x26\n",
      "\tmaxpool_kernel_size: 2\n",
      "\tout_size (after max pool): 13x13\n",
      "conv1 - in_size: 13x13\n",
      "\tin_channels: 8\n",
      "\tout_channels: 16\n",
      "\tkernel_size: 3\n",
      "\tconv_out_size: 11x11\n",
      "\tmaxpool_kernel_size: 2\n",
      "\tout_size (after max pool): 5x5\n",
      "conv2 - in_size: 5x5\n",
      "\tin_channels: 16\n",
      "\tout_channels: 32\n",
      "\tkernel_size: 3\n",
      "\tconv_out_size: 3x3\n",
      "\tmaxpool_kernel_size: 2\n",
      "\tout_size (after max pool): 1x1\n",
      "fc0 - in: 32 - out: 100\n",
      "fc1 - in: 100 - out: 50\n",
      "fc2 - in: 50 - out: 10\n",
      "Diretório de resultados: 'results/generic_1'\n",
      "Model sent to cuda\n",
      "Architecture created\n",
      "Loaders set\n",
      "Train dataset size: 60000\n",
      "Train batch size: 16\n",
      "Modelo 'generic_1' - Parâmetros: 14748\n",
      "Iniciando treinamento do modelo...\n",
      "Random seed set to 42\n",
      "Starting training...\n",
      "Epoch 1/10 | Train loss: 0.98034945 | Val. loss: 0.64316206 | Time: 25.10s | ETA: 3m 45s\n",
      "Epoch 2/10 | Train loss: 0.70167934 | Val. loss: 0.58714514 | Time: 24.62s | ETA: 3m 18s\n",
      "Epoch 3/10 | Train loss: 0.65003271 | Val. loss: 0.56231404 | Time: 25.70s | ETA: 2m 55s\n",
      "Epoch 4/10 | Train loss: 0.61383252 | Val. loss: 0.53559628 | Time: 25.36s | ETA: 2m 31s\n",
      "Epoch 5/10 | Train loss: 0.58781281 | Val. loss: 0.50865372 | Time: 25.48s | ETA: 2m 6s\n",
      "Epoch 6/10 | Train loss: 0.56850202 | Val. loss: 0.49470371 | Time: 24.90s | ETA: 1m 40s\n",
      "Epoch 7/10 | Train loss: 0.54997722 | Val. loss: 0.48550783 | Time: 25.00s | ETA: 1m 15s\n",
      "Epoch 8/10 | Train loss: 0.53569757 | Val. loss: 0.46375825 | Time: 24.91s | ETA: 0m 50s\n",
      "Epoch 9/10 | Train loss: 0.52049131 | Val. loss: 0.45161771 | Time: 25.22s | ETA: 0m 25s\n",
      "Epoch 10/10 | Train loss: 0.50603194 | Val. loss: 0.46371447 | Time: 25.64s | ETA: 0m 0s\n",
      "Training completed in 251.93 seconds.\n",
      "Mean time per epoch: 25.19 seconds.\n",
      "Treinamento completo!\n",
      "Checkpoint saved to results/generic_1/model_checkpoint.pth\n",
      "\n",
      "Checkpoint final salvo em: results/generic_1/model_checkpoint.pth\n",
      "\n",
      "Avaliando o modelo e salvando métricas...\n",
      "Gráfico de perdas salvo em 'results/generic_1/losses.png'\n",
      "Normalized confusion matrix\n",
      "Matriz de confusão salva em 'results/generic_1/confusion_matrix.png'\n",
      "Relatório de classificação:\n",
      "              precision  recall  f1-score     support\n",
      "T-shirt/top    0.832151  0.7040  0.762730   1000.0000\n",
      "Trouser        0.980493  0.9550  0.967579   1000.0000\n",
      "Pullover       0.744658  0.6970  0.720041   1000.0000\n",
      "Dress          0.784050  0.8750  0.827032   1000.0000\n",
      "Coat           0.656352  0.8060  0.723519   1000.0000\n",
      "Sandal         0.950362  0.9190  0.934418   1000.0000\n",
      "Shirt          0.560414  0.4870  0.521134   1000.0000\n",
      "Sneaker        0.930851  0.8750  0.902062   1000.0000\n",
      "Bag            0.939664  0.9500  0.944804   1000.0000\n",
      "Ankle boot     0.877808  0.9770  0.924752   1000.0000\n",
      "accuracy       0.824500  0.8245  0.824500      0.8245\n",
      "macro avg      0.825680  0.8245  0.822807  10000.0000\n",
      "weighted avg   0.825680  0.8245  0.822807  10000.0000\n",
      "Métricas salvas em 'results/generic_1/'\n",
      "Filtros da camada conv_layers.0 salvos em 'results/generic_1/filters_conv_layers.0.png'\n",
      "Filtros da camada conv_layers.1 salvos em 'results/generic_1/filters_conv_layers.1.png'\n",
      "Filtros da camada conv_layers.2 salvos em 'results/generic_1/filters_conv_layers.2.png'\n",
      "Saídas intermediárias salvas em 'results/generic_1/intermediate_outputs.png'\n",
      "\n",
      "Figuras de filtros e ativações salvas em 'results/generic_1/'\n"
     ]
    }
   ],
   "source": [
    "model_generic_1 = GerenicCNN(**example_dicts[1])\n",
    "\n",
    "mflow_generic_1 = ModelFlow(\n",
    "    model=model_generic_1,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    model_name='generic_1'\n",
    ")\n",
    "\n",
    "mflow_generic_1.run(\n",
    "    layers=[f'conv_layers.{i}' for i in range(model_generic_1.amnt_cov_layers)]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 212083,
     "status": "ok",
     "timestamp": 1752153001730,
     "user": {
      "displayName": "Angelo Marcelino",
      "userId": "12130434257979385717"
     },
     "user_tz": 180
    },
    "id": "t6Ytzbd2RDwo",
    "outputId": "cf6dbb11-9782-48c3-ec85-3b02a256fd9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv0 - in_size: 28x28\n",
      "\tin_channels: 1\n",
      "\tout_channels: 4\n",
      "\tkernel_size: 5\n",
      "\tconv_out_size: 24x24\n",
      "\tmaxpool_kernel_size: 2\n",
      "\tout_size (after max pool): 12x12\n",
      "fc0 - in: 576 - out: 20\n",
      "fc1 - in: 20 - out: 10\n",
      "Diretório de resultados: 'results/generic_2'\n",
      "Model sent to cuda\n",
      "Architecture created\n",
      "Loaders set\n",
      "Train dataset size: 60000\n",
      "Train batch size: 16\n",
      "Modelo 'generic_2' - Parâmetros: 11854\n",
      "Iniciando treinamento do modelo...\n",
      "Random seed set to 42\n",
      "Starting training...\n",
      "Epoch 1/10 | Train loss: 0.63731184 | Val. loss: 0.47137132 | Time: 21.30s | ETA: 3m 11s\n",
      "Epoch 2/10 | Train loss: 0.47751705 | Val. loss: 0.42796864 | Time: 20.98s | ETA: 2m 49s\n",
      "Epoch 3/10 | Train loss: 0.44133277 | Val. loss: 0.40466716 | Time: 20.81s | ETA: 2m 27s\n",
      "Epoch 4/10 | Train loss: 0.41619955 | Val. loss: 0.38646013 | Time: 20.96s | ETA: 2m 6s\n",
      "Epoch 5/10 | Train loss: 0.40261364 | Val. loss: 0.37273464 | Time: 20.78s | ETA: 1m 44s\n",
      "Epoch 6/10 | Train loss: 0.38932839 | Val. loss: 0.36578814 | Time: 20.44s | ETA: 1m 23s\n",
      "Epoch 7/10 | Train loss: 0.37787282 | Val. loss: 0.35723087 | Time: 21.01s | ETA: 1m 2s\n",
      "Epoch 8/10 | Train loss: 0.36820805 | Val. loss: 0.35057865 | Time: 21.00s | ETA: 0m 41s\n",
      "Epoch 9/10 | Train loss: 0.36184768 | Val. loss: 0.34752545 | Time: 20.34s | ETA: 0m 20s\n",
      "Epoch 10/10 | Train loss: 0.35598674 | Val. loss: 0.34449877 | Time: 20.96s | ETA: 0m 0s\n",
      "Training completed in 208.60 seconds.\n",
      "Mean time per epoch: 20.86 seconds.\n",
      "Treinamento completo!\n",
      "Checkpoint saved to results/generic_2/model_checkpoint.pth\n",
      "\n",
      "Checkpoint final salvo em: results/generic_2/model_checkpoint.pth\n",
      "\n",
      "Avaliando o modelo e salvando métricas...\n",
      "Gráfico de perdas salvo em 'results/generic_2/losses.png'\n",
      "Normalized confusion matrix\n",
      "Matriz de confusão salva em 'results/generic_2/confusion_matrix.png'\n",
      "Relatório de classificação:\n",
      "              precision  recall  f1-score     support\n",
      "T-shirt/top    0.839248  0.8040  0.821246   1000.0000\n",
      "Trouser        0.990798  0.9690  0.979778   1000.0000\n",
      "Pullover       0.781369  0.8220  0.801170   1000.0000\n",
      "Dress          0.866795  0.8980  0.882122   1000.0000\n",
      "Coat           0.760036  0.8520  0.803395   1000.0000\n",
      "Sandal         0.973711  0.9630  0.968326   1000.0000\n",
      "Shirt          0.710303  0.5860  0.642192   1000.0000\n",
      "Sneaker        0.942073  0.9270  0.934476   1000.0000\n",
      "Bag            0.952941  0.9720  0.962376   1000.0000\n",
      "Ankle boot     0.933462  0.9680  0.950417   1000.0000\n",
      "accuracy       0.876100  0.8761  0.876100      0.8761\n",
      "macro avg      0.875074  0.8761  0.874550  10000.0000\n",
      "weighted avg   0.875074  0.8761  0.874550  10000.0000\n",
      "Métricas salvas em 'results/generic_2/'\n",
      "Filtros da camada conv_layers.0 salvos em 'results/generic_2/filters_conv_layers.0.png'\n",
      "Saídas intermediárias salvas em 'results/generic_2/intermediate_outputs.png'\n",
      "\n",
      "Figuras de filtros e ativações salvas em 'results/generic_2/'\n"
     ]
    }
   ],
   "source": [
    "model_generic_2 = GerenicCNN(**example_dicts[2])\n",
    "\n",
    "mflow_generic_2 = ModelFlow(\n",
    "    model=model_generic_2,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    model_name='generic_2'\n",
    ")\n",
    "\n",
    "mflow_generic_2.run(\n",
    "    layers=[f'conv_layers.{i}' for i in range(model_generic_2.amnt_cov_layers)]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 346012,
     "status": "ok",
     "timestamp": 1752153347746,
     "user": {
      "displayName": "Angelo Marcelino",
      "userId": "12130434257979385717"
     },
     "user_tz": 180
    },
    "id": "MgQZYTU5aQXn",
    "outputId": "e1fd6e20-10ef-42c6-d349-7886f8c72dfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv0 - in_size: 28x28\n",
      "\tin_channels: 1\n",
      "\tout_channels: 5\n",
      "\tkernel_size: 3\n",
      "\tconv_out_size: 26x26\n",
      "\tmaxpool_kernel_size: 2\n",
      "\tout_size (after max pool): 13x13\n",
      "conv1 - in_size: 13x13\n",
      "\tin_channels: 5\n",
      "\tout_channels: 5\n",
      "\tkernel_size: 3\n",
      "\tconv_out_size: 11x11\n",
      "\tmaxpool_kernel_size: 2\n",
      "\tout_size (after max pool): 5x5\n",
      "fc0 - in: 125 - out: 1024\n",
      "fc1 - in: 1024 - out: 899\n",
      "fc2 - in: 899 - out: 774\n",
      "fc3 - in: 774 - out: 649\n",
      "fc4 - in: 649 - out: 524\n",
      "fc5 - in: 524 - out: 399\n",
      "fc6 - in: 399 - out: 274\n",
      "fc7 - in: 274 - out: 149\n",
      "fc8 - in: 149 - out: 25\n",
      "fc9 - in: 25 - out: 10\n",
      "Diretório de resultados: 'results/generic_3'\n",
      "Model sent to cuda\n",
      "Architecture created\n",
      "Loaders set\n",
      "Train dataset size: 60000\n",
      "Train batch size: 16\n",
      "Modelo 'generic_3' - Parâmetros: 2955014\n",
      "Iniciando treinamento do modelo...\n",
      "Random seed set to 42\n",
      "Starting training...\n",
      "Epoch 1/10 | Train loss: 1.10237083 | Val. loss: 0.64863194 | Time: 33.58s | ETA: 5m 2s\n",
      "Epoch 2/10 | Train loss: 0.68976186 | Val. loss: 0.57120443 | Time: 33.79s | ETA: 4m 29s\n",
      "Epoch 3/10 | Train loss: 0.61367610 | Val. loss: 0.51491254 | Time: 34.02s | ETA: 3m 56s\n",
      "Epoch 4/10 | Train loss: 0.57178831 | Val. loss: 0.47043421 | Time: 33.62s | ETA: 3m 22s\n",
      "Epoch 5/10 | Train loss: 0.54788325 | Val. loss: 0.46485210 | Time: 34.25s | ETA: 2m 49s\n",
      "Epoch 6/10 | Train loss: 0.52534629 | Val. loss: 0.46257663 | Time: 34.04s | ETA: 2m 15s\n",
      "Epoch 7/10 | Train loss: 0.51129600 | Val. loss: 0.47005113 | Time: 33.86s | ETA: 1m 41s\n",
      "Epoch 8/10 | Train loss: 0.49576140 | Val. loss: 0.41532328 | Time: 33.17s | ETA: 1m 7s\n",
      "Epoch 9/10 | Train loss: 0.48559011 | Val. loss: 0.41241012 | Time: 33.63s | ETA: 0m 33s\n",
      "Epoch 10/10 | Train loss: 0.47509677 | Val. loss: 0.41195066 | Time: 33.79s | ETA: 0m 0s\n",
      "Training completed in 337.75 seconds.\n",
      "Mean time per epoch: 33.77 seconds.\n",
      "Treinamento completo!\n",
      "Checkpoint saved to results/generic_3/model_checkpoint.pth\n",
      "\n",
      "Checkpoint final salvo em: results/generic_3/model_checkpoint.pth\n",
      "\n",
      "Avaliando o modelo e salvando métricas...\n",
      "Gráfico de perdas salvo em 'results/generic_3/losses.png'\n",
      "Normalized confusion matrix\n",
      "Matriz de confusão salva em 'results/generic_3/confusion_matrix.png'\n",
      "Relatório de classificação:\n",
      "              precision  recall  f1-score     support\n",
      "T-shirt/top    0.789738  0.7850  0.787362   1000.0000\n",
      "Trouser        0.995876  0.9660  0.980711   1000.0000\n",
      "Pullover       0.791222  0.6670  0.723820   1000.0000\n",
      "Dress          0.870871  0.8700  0.870435   1000.0000\n",
      "Coat           0.594185  0.9400  0.728118   1000.0000\n",
      "Sandal         0.965760  0.9590  0.962368   1000.0000\n",
      "Shirt          0.626712  0.3660  0.462121   1000.0000\n",
      "Sneaker        0.935976  0.9210  0.928427   1000.0000\n",
      "Bag            0.951076  0.9720  0.961424   1000.0000\n",
      "Ankle boot     0.935860  0.9630  0.949236   1000.0000\n",
      "accuracy       0.840900  0.8409  0.840900      0.8409\n",
      "macro avg      0.845728  0.8409  0.835402  10000.0000\n",
      "weighted avg   0.845728  0.8409  0.835402  10000.0000\n",
      "Métricas salvas em 'results/generic_3/'\n",
      "Filtros da camada conv_layers.0 salvos em 'results/generic_3/filters_conv_layers.0.png'\n",
      "Filtros da camada conv_layers.1 salvos em 'results/generic_3/filters_conv_layers.1.png'\n",
      "Saídas intermediárias salvas em 'results/generic_3/intermediate_outputs.png'\n",
      "\n",
      "Figuras de filtros e ativações salvas em 'results/generic_3/'\n"
     ]
    }
   ],
   "source": [
    "model_generic_3 = GerenicCNN(**example_dicts[3])\n",
    "\n",
    "mflow_generic_3 = ModelFlow(\n",
    "    model=model_generic_3,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    model_name='generic_3'\n",
    ")\n",
    "\n",
    "mflow_generic_3.run(\n",
    "    layers=[f'conv_layers.{i}' for i in range(model_generic_3.amnt_cov_layers)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qfq4_1tprkPj"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMhOGytdj5CIMDYFRbjCpNf",
   "collapsed_sections": [
    "_RBKS9fnYUfn",
    "ZSech2uFZZVj",
    "MJIlh3GhvU-T",
    "aecieQzk0ena",
    "XiWmKH4CZmdy",
    "t-jrnp1uYjqE",
    "0QDz6_gIfehS",
    "cw_2zOkdi7hS",
    "l5YuSpDH3Scb",
    "TVELNv_8-apz",
    "vr_EzTAEQJrC"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
