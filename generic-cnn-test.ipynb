{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPp+Hpb7Iq61ZadAuliLV9i"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Arch"],"metadata":{"id":"QoQa15PLjqd9"}},{"cell_type":"code","source":["import time\n","import torch\n","import torch.nn as nn\n","import numpy as np\n","import random\n","import matplotlib.pyplot as plt\n","import torch.nn.functional as F\n","\n","from torchvision.transforms.v2 import Normalize\n","\n","class Architecture(object):\n","    def __init__(self, model, loss_fn, optimizer, verbose=True, class_names=None):\n","        # Here we define the attributes of our class\n","        self.verbose=verbose\n","        self.class_names = class_names\n","\n","        # We start by storing the arguments as attributes\n","        # to use them later\n","        self.model = model\n","        self.loss_fn = loss_fn\n","        self.optimizer = optimizer\n","        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","        # Let's send the model to the specified device right away\n","        self.model.to(self.device)\n","        if self.verbose:\n","            print(f\"Model sent to {self.device}\")\n","\n","        # These attributes are defined here, but since they are\n","        # not informed at the moment of creation, we keep them None\n","        self.train_loader = None\n","        self.val_loader = None\n","\n","        # These attributes are going to be computed internally\n","        self.losses = []\n","        self.val_losses = []\n","        self.total_epochs = 0\n","\n","        # Creates the train_step function for our model,\n","        # loss function and optimizer\n","        # Note: there are NO ARGS there! It makes use of the class\n","        # attributes directly\n","        self.train_step_fn = self._make_train_step_fn()\n","        # Creates the val_step function for our model and loss\n","        self.val_step_fn = self._make_val_step_fn()\n","\n","        # for hook purposes\n","        self.handles = {}\n","        self.visualization = {}\n","\n","        if self.verbose:\n","            print(\"Architecture created\")\n","\n","    def to(self, device):\n","        # This method allows the user to specify a different device\n","        # It sets the corresponding attribute (to be used later in\n","        # the mini-batches) and sends the model to the device\n","        try:\n","            self.device = device\n","            self.model.to(self.device)\n","            if self.verbose:\n","                print(f\"Model sent to {device}\")\n","        except RuntimeError:\n","            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","            print(f\"Couldn't send it to {device}, sending it to {self.device} instead.\")\n","            self.model.to(self.device)\n","\n","    def set_loaders(self, train_loader, val_loader=None):\n","        # This method allows the user to define which train_loader (and val_loader, optionally) to use\n","        # Both loaders are then assigned to attributes of the class\n","        # So they can be referred to later\n","        self.train_loader = train_loader\n","        self.val_loader = val_loader\n","\n","        if self.verbose:\n","            # Print train loader info\n","            print(\"Loaders set\")\n","            if self.train_loader.dataset:\n","                print(f\"Train dataset size: {len(self.train_loader.dataset)}\")\n","            if self.train_loader.batch_size:\n","                print(f\"Train batch size: {self.train_loader.batch_size}\")\n","\n","    def _make_train_step_fn(self):\n","        # This method does not need ARGS... it can refer to\n","        # the attributes: self.model, self.loss_fn and self.optimizer\n","\n","        # Builds function that performs a step in the train loop\n","        def perform_train_step_fn(x, y):\n","            # Sets model to TRAIN mode\n","            self.model.train()\n","\n","            # Step 1 - Computes our model's predicted output - forward pass\n","            yhat = self.model(x)\n","            # Step 2 - Computes the loss\n","            loss = self.loss_fn(yhat, y)\n","            # Step 3 - Computes gradients for both \"a\" and \"b\" parameters\n","            loss.backward()\n","            # Step 4 - Updates parameters using gradients and the learning rate\n","            self.optimizer.step()\n","            self.optimizer.zero_grad()\n","\n","            # Returns the loss\n","            return loss.item()\n","\n","        # Returns the function that will be called inside the train loop\n","        return perform_train_step_fn\n","\n","    def _make_val_step_fn(self):\n","        # Builds function that performs a step in the validation loop\n","        def perform_val_step_fn(x, y):\n","            # Sets model to EVAL mode\n","            self.model.eval()\n","\n","            # Step 1 - Computes our model's predicted output - forward pass\n","            yhat = self.model(x)\n","            # Step 2 - Computes the loss\n","            loss = self.loss_fn(yhat, y)\n","            # There is no need to compute Steps 3 and 4, since we don't update parameters during evaluation\n","            return loss.item()\n","\n","        return perform_val_step_fn\n","\n","    def _mini_batch(self, validation=False, verbose_mini_batch=None, mini_batch_report = 100):\n","        # The mini-batch can be used with both loaders\n","        # The argument `validation`defines which loader and\n","        # corresponding step function is going to be used\n","        if validation:\n","            data_loader = self.val_loader\n","            step_fn = self.val_step_fn\n","        else:\n","            data_loader = self.train_loader\n","            step_fn = self.train_step_fn\n","\n","        if data_loader is None:\n","            return None\n","\n","        local_verbose = verbose_mini_batch and self.verbose\n","\n","        if local_verbose:\n","            print(\"\\tStarting mini-batch...\")\n","\n","        # Once the data loader and step function, this is the same\n","        # mini-batch loop we had before\n","        mini_batch_losses = []\n","        count = 0\n","        for x_batch, y_batch in data_loader:\n","            x_batch = x_batch.to(self.device)\n","            y_batch = y_batch.to(self.device)\n","\n","            mini_batch_loss = step_fn(x_batch, y_batch)\n","            mini_batch_losses.append(mini_batch_loss)\n","\n","            if local_verbose and count % mini_batch_report == 0:\n","                print(f\"\\t\\tBatch {count}/{len(data_loader)}, loss: {mini_batch_loss}\")\n","\n","        loss = np.mean(mini_batch_losses)\n","        return loss\n","\n","    # this function was updated in this class\n","    def set_seed(self, seed=42):\n","        torch.backends.cudnn.deterministic = True\n","        torch.backends.cudnn.benchmark = False\n","        torch.manual_seed(seed)\n","        np.random.seed(seed)\n","        random.seed(seed)\n","        try:\n","            self.train_loader.sampler.generator.manual_seed(seed)\n","        except AttributeError:\n","            pass\n","        if self.verbose:\n","            print(f\"Random seed set to {seed}\")\n","\n","    def train(self, n_epochs, seed=42, verbose=None, verbose_mini_batch=False, mini_batch_report=100, batch_report=10):\n","        # To ensure reproducibility of the training process\n","        self.set_seed(seed)\n","\n","        local_verbose = self.verbose if verbose is None else verbose\n","        epoch_times = []\n","\n","        if local_verbose:\n","            print(\"Starting training...\")\n","            global_start = time.time()\n","\n","\n","        for epoch in range(n_epochs):\n","            # Keeps track of the numbers of epochs\n","            # by updating the corresponding attribute\n","            epoch_start = time.time()\n","            self.total_epochs += 1\n","\n","            # inner loop\n","            # Performs training using mini-batches\n","            loss = self._mini_batch(validation=False,\n","                                    verbose_mini_batch=verbose_mini_batch,\n","                                    mini_batch_report=mini_batch_report)\n","            self.losses.append(loss)\n","\n","            # VALIDATION\n","            # no gradients in validation!\n","            with torch.no_grad():\n","                # Performs evaluation using mini-batches\n","                val_loss = self._mini_batch(validation=True)\n","                self.val_losses.append(val_loss)\n","\n","            epoch_end = time.time()\n","            elapsed = epoch_end - epoch_start\n","            epoch_times.append(elapsed)\n","\n","            is_first_epoch = epoch == 0\n","            is_last_epoch = epoch == n_epochs - 1\n","\n","            has_report = is_first_epoch or is_last_epoch or (epoch + 1) % batch_report == 0\n","\n","            if local_verbose and has_report:\n","                # Predict remaining duration\n","                avg_time = sum(epoch_times) / len(epoch_times)\n","                remaining_epochs = n_epochs - (epoch + 1)\n","                estimated_remaining_secs = avg_time * remaining_epochs\n","                mins, secs = divmod(int(estimated_remaining_secs), 60)\n","\n","                print(\n","                    f\"Epoch {self.total_epochs}/{n_epochs} | \"\n","                    f\"Train loss: {loss:.8f} | Val. loss: {val_loss:.8f} | \"\n","                    f\"Time: {elapsed:.2f}s | ETA: {mins}m {secs}s\"\n","                )\n","\n","        if local_verbose:\n","            total_time = time.time() - global_start\n","            mean_epoch_time = sum(epoch_times) / len(epoch_times)\n","            print(f\"Training completed in {total_time:.2f} seconds.\")\n","            print(f\"Mean time per epoch: {mean_epoch_time:.2f} seconds.\")\n","\n","    def save_checkpoint(self, filename):\n","        # Builds dictionary with all elements for resuming training\n","        checkpoint = {'epoch': self.total_epochs,\n","                      'model_state_dict': self.model.state_dict(),\n","                      'optimizer_state_dict': self.optimizer.state_dict(),\n","                      'loss': self.losses,\n","                      'val_loss': self.val_losses}\n","\n","        torch.save(checkpoint, filename)\n","        if self.verbose:\n","            print(f\"Checkpoint saved to {filename}\")\n","\n","    def load_checkpoint(self, filename):\n","        # Loads dictionary\n","        checkpoint = torch.load(filename)\n","\n","        # Restore state for model and optimizer\n","        self.model.load_state_dict(checkpoint['model_state_dict'])\n","        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","\n","        self.total_epochs = checkpoint['epoch']\n","        self.losses = checkpoint['loss']\n","        self.val_losses = checkpoint['val_loss']\n","\n","        self.model.train() # always use TRAIN for resuming training\n","\n","        if self.verbose:\n","            print(f\"Checkpoint loaded from {filename}\")\n","            print(checkpoint)\n","\n","    def predict(self, x):\n","        # Set is to evaluation mode for predictions\n","        self.model.eval()\n","        # Takes aNumpy input and make it a float tensor\n","        x_tensor = torch.as_tensor(x).float()\n","        # Send input to device and uses model for prediction\n","        y_hat_tensor = self.model(x_tensor.to(self.device))\n","        # Set it back to train mode\n","        self.model.train()\n","        # Detaches it, brings it to CPU and back to Numpy\n","        return y_hat_tensor.detach().cpu().numpy()\n","\n","    def count_parameters(self):\n","      return sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n","\n","    def plot_losses(self):\n","        fig = plt.figure(figsize=(10, 4))\n","        plt.plot(self.losses, label='Training Loss', c='b')\n","        plt.plot(self.val_losses, label='Validation Loss', c='r')\n","        plt.yscale('log')\n","        plt.xlabel('Epochs')\n","        plt.ylabel('Loss')\n","        plt.legend()\n","        plt.tight_layout()\n","        return fig\n","\n","    @staticmethod\n","    def _visualize_tensors(axs, x, y=None, yhat=None, layer_name='', title=None, class_names=None):\n","        # The number of images is the number of subplots in a row\n","        n_images = len(axs)\n","        # Gets max and min values for scaling the grayscale\n","        minv, maxv = np.min(x[:n_images]), np.max(x[:n_images])\n","        # For each image\n","        for j, image in enumerate(x[:n_images]):\n","            ax = axs[j]\n","            # Sets title, labels, and removes ticks\n","            if title is not None:\n","                ax.set_title(f'{title} #{j}', fontsize=12)\n","            shp = np.atleast_2d(image).shape\n","            ax.set_ylabel(\n","                f'{layer_name}\\n{shp[0]}x{shp[1]}',\n","                rotation=0, labelpad=40\n","            )\n","\n","            if class_names is not None:\n","                xlabel1 = '' if y is None else f'\\nReal: {class_names[y[j]]}'\n","                xlabel2 = '' if yhat is None else f'\\nPred.: {class_names[yhat[j]]}'\n","            else:\n","                xlabel1 = '' if y is None else f'\\nLabel: {y[j]}'\n","                xlabel2 = '' if yhat is None else f'\\nPredicted: {yhat[j]}'\n","\n","            xlabel = f'{xlabel1}{xlabel2}'\n","            if len(xlabel):\n","                ax.set_xlabel(xlabel, fontsize=12)\n","            ax.set_xticks([])\n","            ax.set_yticks([])\n","\n","            # Plots weight as an image\n","            ax.imshow(\n","                np.atleast_2d(image.squeeze()),\n","                cmap='gray',\n","                vmin=minv,\n","                vmax=maxv\n","            )\n","        return\n","\n","    def visualize_filters(self, layer_name, **kwargs):\n","        try:\n","            # Gets the layer object from the model\n","            layer = self.model\n","            for name in layer_name.split('.'):\n","                layer = getattr(layer, name)\n","            # We are only looking at filters for 2D convolutions\n","            if isinstance(layer, nn.Conv2d):\n","                # Takes the weight information\n","                weights = layer.weight.data.cpu().numpy()\n","                # weights -> (channels_out (filter), channels_in, H, W)\n","                n_filters, n_channels, _, _ = weights.shape\n","\n","                # Builds a figure\n","                size = (2 * n_channels + 2, 2 * n_filters)\n","                fig, axes = plt.subplots(n_filters, n_channels,\n","                                        figsize=size)\n","                axes = np.atleast_2d(axes)\n","                axes = axes.reshape(n_filters, n_channels)\n","                # For each channel_out (filter)\n","                for i in range(n_filters):\n","                    Architecture._visualize_tensors(\n","                        axes[i, :],\n","                        weights[i],\n","                        layer_name=f'Filter #{i}',\n","                        title='Channel',\n","                    )\n","\n","                for ax in axes.flat:\n","                    ax.label_outer()\n","\n","                fig.tight_layout()\n","                return fig\n","        except AttributeError:\n","            return\n","\n","    def attach_hooks(self, layers_to_hook, hook_fn=None):\n","        # Clear any previous values\n","        self.visualization = {}\n","        # Creates the dictionary to map layer objects to their names\n","        modules = list(self.model.named_modules())\n","        layer_names = {layer: name for name, layer in modules[1:]}\n","\n","        if hook_fn is None:\n","            # Hook function to be attached to the forward pass\n","            def hook_fn(layer, inputs, outputs):\n","                # Gets the layer name\n","                name = layer_names[layer]\n","                # Detaches outputs\n","                values = outputs.detach().cpu().numpy()\n","                # Since the hook function may be called multiple times\n","                # for example, if we make predictions for multiple mini-batches\n","                # it concatenates the results\n","                if self.visualization[name] is None:\n","                    self.visualization[name] = values\n","                else:\n","                    self.visualization[name] = np.concatenate([self.visualization[name], values])\n","\n","        for name, layer in modules:\n","            # If the layer is in our list\n","            if name in layers_to_hook:\n","                # Initializes the corresponding key in the dictionary\n","                self.visualization[name] = None\n","                # Register the forward hook and keep the handle in another dict\n","                self.handles[name] = layer.register_forward_hook(hook_fn)\n","\n","    def remove_hooks(self):\n","        # Loops through all hooks and removes them\n","        for handle in self.handles.values():\n","            handle.remove()\n","        # Clear the dict, as all hooks have been removed\n","        self.handles = {}\n","\n","    def visualize_outputs(self, layers, n_images=10, y=None, yhat=None):\n","        layers = filter(lambda l: l in self.visualization.keys(), layers)\n","        layers = list(layers)\n","        shapes = [self.visualization[layer].shape for layer in layers]\n","        n_rows = [shape[1] if len(shape) == 4 else 1\n","                  for shape in shapes]\n","        total_rows = np.sum(n_rows)\n","\n","        fig, axes = plt.subplots(total_rows, n_images,\n","                                figsize=(1.5*n_images, 1.5*total_rows))\n","        axes = np.atleast_2d(axes).reshape(total_rows, n_images)\n","\n","        # Loops through the layers, one layer per row of subplots\n","        row = 0\n","        for i, layer in enumerate(layers):\n","            start_row = row\n","            # Takes the produced feature maps for that layer\n","            output = self.visualization[layer]\n","\n","            is_vector = len(output.shape) == 2\n","\n","            for j in range(n_rows[i]):\n","                Architecture._visualize_tensors(\n","                    axes[row, :],\n","                    output if is_vector else output[:, j].squeeze(),\n","                    y,\n","                    yhat,\n","                    layer_name=layers[i] \\\n","                              if is_vector \\\n","                              else f'{layers[i]}\\nfil#{row-start_row}',\n","                    title='Image' if (row == 0) else None,\n","                    class_names=self.class_names\n","                )\n","                row += 1\n","\n","        for ax in axes.flat:\n","            ax.label_outer()\n","\n","        plt.tight_layout()\n","        return fig\n","\n","    def correct(self, x, y, threshold=.5):\n","        self.model.eval()\n","        yhat = self.model(x.to(self.device))\n","        y = y.to(self.device)\n","        self.model.train()\n","\n","        # We get the size of the batch and the number of classes\n","        # (only 1, if it is binary)\n","        n_samples, n_dims = yhat.shape\n","        if n_dims > 1:\n","            # In a multiclass classification, the biggest logit\n","            # always wins, so we don't bother getting probabilities\n","\n","            # This is PyTorch's version of argmax,\n","            # but it returns a tuple: (max value, index of max value)\n","            _, predicted = torch.max(yhat, 1)\n","        else:\n","            n_dims += 1\n","            # In binary classification, we NEED to check if the\n","            # last layer is a sigmoid (and then it produces probs)\n","            if isinstance(self.model, nn.Sequential) and \\\n","              isinstance(self.model[-1], nn.Sigmoid):\n","                predicted = (yhat > threshold).long()\n","            # or something else (logits), which we need to convert\n","            # using a sigmoid\n","            else:\n","                predicted = (F.sigmoid(yhat) > threshold).long()\n","\n","        # How many samples got classified correctly for each class\n","        result = []\n","        for c in range(n_dims):\n","            n_class = (y == c).sum().item()\n","            n_correct = (predicted[y == c] == c).sum().item()\n","            result.append((n_correct, n_class))\n","        return torch.tensor(result)\n","\n","\n","    @staticmethod\n","    def loader_apply(loader, func, reduce='sum'):\n","        results = [func(x, y) for i, (x, y) in enumerate(loader)]\n","        results = torch.stack(results, axis=0)\n","\n","        if reduce == 'sum':\n","            results = results.sum(axis=0)\n","        elif reduce == 'mean':\n","            results = results.float().mean(axis=0)\n","\n","        return results\n","\n","    @staticmethod\n","    def statistics_per_channel(images, labels):\n","        # NCHW\n","        n_samples, n_channels, n_height, n_weight = images.size()\n","        # Flatten HW into a single dimension\n","        flatten_per_channel = images.reshape(n_samples, n_channels, -1)\n","\n","        # Computes statistics of each image per channel\n","        # Average pixel value per channel\n","        # (n_samples, n_channels)\n","        means = flatten_per_channel.mean(axis=2)\n","        # Standard deviation of pixel values per channel\n","        # (n_samples, n_channels)\n","        stds = flatten_per_channel.std(axis=2)\n","\n","        # Adds up statistics of all images in a mini-batch\n","        # (1, n_channels)\n","        sum_means = means.sum(axis=0)\n","        sum_stds = stds.sum(axis=0)\n","        # Makes a tensor of shape (1, n_channels)\n","        # with the number of samples in the mini-batch\n","        n_samples = torch.tensor([n_samples]*n_channels).float()\n","\n","        # Stack the three tensors on top of one another\n","        # (3, n_channels)\n","        return torch.stack([n_samples, sum_means, sum_stds], axis=0)\n","\n","    @staticmethod\n","    def make_normalizer(loader):\n","        total_samples, total_means, total_stds = Architecture.loader_apply(loader, Architecture.statistics_per_channel)\n","        norm_mean = total_means / total_samples\n","        norm_std = total_stds / total_samples\n","        return Normalize(mean=norm_mean, std=norm_std)\n","\n","    def set_optimizer(self, optimizer):\n","        self.optimizer = optimizer"],"metadata":{"id":"KABVU0lRjhJA","executionInfo":{"status":"ok","timestamp":1752149771017,"user_tz":180,"elapsed":8345,"user":{"displayName":"Angelo Marcelino","userId":"12130434257979385717"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["#  Generic CNN Model"],"metadata":{"id":"zd81kyMkj5J5"}},{"cell_type":"code","source":["import math\n","\n","class GerenicCNN(nn.Module):\n","    def __init__(self,\n","                 in_channels,\n","                 num_classes,\n","                 input_size=28,\n","                 amnt_cov_layers=2,\n","                 cov_kernel_sizes=None,\n","                 cov_features=None,\n","                 max_pool_sizes=None,\n","                 amnt_fc_layers=2,\n","                 fc_sizes=None,\n","                 p=0.3):\n","        super(GerenicCNN, self).__init__()\n","\n","        # Define defaults se não fornecidos\n","        if cov_kernel_sizes is None:\n","            cov_kernel_sizes = [3] * amnt_cov_layers\n","        if cov_features is None:\n","            cov_features = [16] * amnt_cov_layers\n","        if max_pool_sizes is None:\n","            max_pool_sizes = [2] * amnt_cov_layers\n","        if fc_sizes is None:\n","            fc_sizes = [50] * (amnt_fc_layers - 1)  # só intermediários\n","\n","        # Validação\n","        self._validate_parameters(amnt_cov_layers,\n","                                 cov_kernel_sizes,\n","                                 cov_features,\n","                                 max_pool_sizes,\n","                                 amnt_fc_layers,\n","                                 fc_sizes)\n","\n","        self.in_channels = in_channels\n","        self.num_classes = num_classes\n","        self.input_size = input_size\n","        self.amnt_cov_layers = amnt_cov_layers\n","        self.cov_kernel_sizes = cov_kernel_sizes\n","        self.cov_features = cov_features\n","        self.max_pool_sizes = max_pool_sizes\n","        self.amnt_fc_layers = amnt_fc_layers\n","        self.fc_sizes = fc_sizes\n","        self.p = p\n","\n","        # Criação das camadas convolucionais e maxPool\n","        self.conv_layers = nn.ModuleList()\n","        self.max_pool_layers = nn.ModuleList()\n","\n","        current_channels = in_channels\n","        current_size = input_size\n","        for i in range(amnt_cov_layers):\n","\n","            if current_size <= 1:\n","                msg = f'Não será possível criar a camada convolucional {i}, '\n","                msg += \"pois não será possível aplicar MaxPool de pelo menos 2.\"\n","                raise ValueError(msg)\n","\n","            self.conv_layers.append(\n","                nn.Conv2d(\n","                    in_channels=current_channels,\n","                    out_channels=cov_features[i],\n","                    kernel_size=cov_kernel_sizes[i]\n","                )\n","            )\n","\n","            self.max_pool_layers.append(\n","                nn.MaxPool2d(\n","                    kernel_size=max_pool_sizes[i]\n","                )\n","            )\n","            conv_out_size, out_size = self._compute_next_dims(current_size, cov_kernel_sizes[i], max_pool_sizes[i])\n","\n","            print(f'conv{i}', end=' - ')\n","            print(f'in_size: {current_size}x{current_size}', end='\\n\\t')\n","            print(f'in_channels: {current_channels}', end='\\n\\t')\n","            print(f'out_channels: {cov_features[i]}', end='\\n\\t')\n","            print(f'kernel_size: {cov_kernel_sizes[i]}', end='\\n\\t')\n","            print(f'conv_out_size: {conv_out_size}x{conv_out_size}', end='\\n\\t')\n","            print(f'maxpool_kernel_size: {max_pool_sizes[i]}', end='\\n\\t')\n","            print(f'out_size (after max pool): {out_size}x{out_size}')\n","\n","\n","            current_channels = cov_features[i]\n","            current_size = out_size\n","\n","            if current_size < 1:\n","                msg = f'Erro! Saída da camada convolucional {i} calculada com dimensão 0'\n","                raise ValueError(msg)\n","\n","\n","        # Calcula flattened_dim para o fc\n","        # Criar um tensor de exemplo para calcular a dimensão flattened\n","        with torch.no_grad():\n","            dummy_input = torch.randn(1, in_channels, input_size, input_size)\n","            dummy_output = self.featurizer(dummy_input)\n","            self.flattened_dim = dummy_output.numel() // dummy_output.shape[0]\n","\n","\n","        # Criação das camadas fully connected\n","        self.fc_layers = nn.ModuleList()\n","\n","        full_fc_sizes = [self.flattened_dim] + self.fc_sizes + [num_classes]\n","\n","        for i in range(amnt_fc_layers):\n","            print(f'fc{i} - in: {full_fc_sizes[i]} - out: {full_fc_sizes[i + 1]}')\n","            self.fc_layers.append(\n","                nn.Linear(\n","                    in_features=full_fc_sizes[i],\n","                    out_features=full_fc_sizes[i + 1]\n","                )\n","            )\n","\n","        self.drop = nn.Dropout(self.p)\n","\n","    def _validate_parameters(self, amnt_cov_layers, cov_kernel_sizes, cov_features, max_pool_sizes,\n","                        amnt_fc_layers, fc_sizes):\n","        assert len(cov_kernel_sizes) == amnt_cov_layers, \\\n","            f\"'cov_kernel_sizes' deve ter {amnt_cov_layers} elementos, mas tem {len(cov_kernel_sizes)}.\"\n","        assert len(cov_features) == amnt_cov_layers, \\\n","            f\"'cov_features' deve ter {amnt_cov_layers} elementos, mas tem {len(cov_features)}.\"\n","        assert len(max_pool_sizes) == amnt_cov_layers, \\\n","            f\"'max_pool_sizes' deve ter {amnt_cov_layers} elementos, mas tem {len(max_pool_sizes)}.\"\n","        assert len(fc_sizes) == (amnt_fc_layers - 1), \\\n","            f\"'fc_sizes' deve ter {amnt_fc_layers - 1} elementos, mas tem {len(fc_sizes)}.\"\n","\n","\n","    def _compute_next_dims(self, current_dim, kernel_size, max_pool_size):\n","\n","        # Cálculo é [(w - k + 2p) / s] + 1\n","        # Por padrão, o padding é 0 e o stride é 1\n","        # Então o cálculo fica = [(w - k) / 1] + 1\n","        conv_dim = (current_dim - kernel_size) + 1\n","\n","\n","        # Por padrão, o padding é 0 e o stride é o kernel size\n","        # Então o cálculo fica = [(w - k) / k] + 1\n","        # = [(w / k) - 1] + 1\n","        # = w / k\n","        after_pool_dim = conv_dim / max_pool_size\n","\n","        # Arredonda para baixo\n","        after_pool_dim = math.floor(after_pool_dim)\n","        return conv_dim, after_pool_dim\n","\n","    def featurizer(self, x):\n","        # for i, conv_layer in enumerate(self.conv_layers):\n","        #     x = conv_layer(x)\n","        #     x = F.relu(x)\n","        #     x = F.max_pool2d(x, kernel_size=self.max_pool_sizes[i])\n","        # x = torch.flatten(x, 1)  # flatten exceto a batch dimension\n","\n","        for conv_layer, max_pool_layer in zip(self.conv_layers, self.max_pool_layers):\n","            x = conv_layer(x)\n","            x = F.relu(x)\n","            x = max_pool_layer(x)\n","\n","        return x\n","\n","\n","    def classifier(self, x):\n","        x = torch.flatten(x, 1)\n","        for i, fc_layer in enumerate(self.fc_layers):\n","            if self.p > 0 and i < len(self.fc_layers) - 1: # Apply dropout only before non-output layers\n","                x = self.drop(x)\n","            x = fc_layer(x)\n","            # Aplica ReLU em todas as camadas, exceto a última\n","            if i < len(self.fc_layers) - 1:\n","                x = F.relu(x)\n","        return x\n","\n","    def forward(self, x):\n","        x = self.featurizer(x)\n","        x = self.classifier(x)\n","        return x"],"metadata":{"id":"5AlqIfW5kAs1","executionInfo":{"status":"ok","timestamp":1752149771018,"user_tz":180,"elapsed":6,"user":{"displayName":"Angelo Marcelino","userId":"12130434257979385717"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["# Metrics"],"metadata":{"id":"E2miwrJEjymg"}},{"cell_type":"code","source":["from sklearn.metrics import confusion_matrix\n","import itertools\n","\n","def get_all_predictions(model, loader, device):\n","    all_preds = torch.tensor([]).to(device)\n","    all_labels = torch.tensor([]).to(device)\n","    for images, labels in loader:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        preds = model(images)\n","        all_preds = torch.cat(\n","            (all_preds, preds)\n","            , dim=0\n","        )\n","        all_labels = torch.cat(\n","            (all_labels, labels)\n","            , dim=0\n","        )\n","    return all_preds.cpu(), all_labels.cpu()\n","\n","def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n","    if normalize:\n","        cm = (cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] ) * 100.0\n","        print(\"Normalized confusion matrix\")\n","    else:\n","        print('Confusion matrix, without normalization')\n","\n","    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","    plt.title(title)\n","    plt.colorbar()\n","    tick_marks = np.arange(len(classes))\n","    plt.xticks(tick_marks, classes, rotation=45)\n","    plt.yticks(tick_marks, classes)\n","\n","    plt.grid(False)\n","\n","    fmt = '.2f' if normalize else 'd'\n","    thresh = cm.max() / 2.\n","    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","        if normalize and cm[i, j] == 0:\n","            continue\n","        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n","\n","    plt.tight_layout()\n","    plt.ylabel('True label')\n","    plt.xlabel('Predicted label')\n","\n","    return plt"],"metadata":{"id":"sAWNXnqFj2GD","executionInfo":{"status":"ok","timestamp":1752149773590,"user_tz":180,"elapsed":2575,"user":{"displayName":"Angelo Marcelino","userId":"12130434257979385717"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["# Flow"],"metadata":{"id":"YWtq-VH8zPKY"}},{"cell_type":"code","source":["from sklearn.metrics import confusion_matrix, classification_report\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import os\n","import torch.optim as optim  # Optimization algorithms\n","\n","class ModelFlow():\n","\n","    def __init__(self, model, train_loader, val_loader, model_name=\"model_instance\", load_checkpoint_path=None):\n","        self.model = model\n","        self.model_name = model_name\n","        self.train_loader = train_loader\n","        self.val_loader = val_loader\n","\n","        self.results_dir = f\"results/{self.model_name}\"\n","        os.makedirs(self.results_dir, exist_ok=True)\n","        print(f\"Diretório de resultados: '{self.results_dir}'\")\n","\n","        torch.manual_seed(13)\n","\n","        self.multi_loss_fn = nn.CrossEntropyLoss(reduction='mean')\n","        self.optimizer = optim.Adam(model.parameters(), lr=3e-4)\n","\n","        self.class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n","                           'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n","\n","        self.arch = Architecture(self.model,\n","                                 self.multi_loss_fn,\n","                                 self.optimizer,\n","                                 class_names=self.class_names,\n","                                 verbose=True)\n","        self.arch.set_loaders(self.train_loader, self.val_loader)\n","\n","        if load_checkpoint_path:\n","            print(f\"Carregando checkpoint de: {load_checkpoint_path}\")\n","            self.arch.load_checkpoint(load_checkpoint_path)\n","\n","        print(f\"Modelo '{self.model_name}' - Parâmetros: {self.arch.count_parameters()}\")\n","\n","    def train(self, epochs=10):\n","        print(\"Iniciando treinamento do modelo...\")\n","        self.arch.train(epochs, verbose=True, verbose_mini_batch=False, batch_report=1)\n","        print(\"Treinamento completo!\")\n","\n","    def evaluate(self):\n","        print(\"\\nAvaliando o modelo e salvando métricas...\")\n","        # Gráfico de Perdas\n","        losses_fig = self.arch.plot_losses()\n","        losses_fig.savefig(f\"{self.results_dir}/losses.png\")\n","        plt.close(losses_fig)\n","        print(f\"Gráfico de perdas salvo em '{self.results_dir}/losses.png'\")\n","\n","        # Matriz de Confusão\n","        with torch.no_grad():\n","            val_preds, val_labels = get_all_predictions(self.model, self.val_loader, self.arch.device)\n","        cm = confusion_matrix(val_labels.numpy(), val_preds.argmax(dim=1).numpy())\n","        fig_cm = plt.figure(figsize=(10,10))\n","        plot_confusion_matrix(cm, self.class_names, normalize=True, title=f\"Matriz de Confusão - {self.model_name}\")\n","        fig_cm.savefig(f\"{self.results_dir}/confusion_matrix.png\")\n","        plt.close(fig_cm)\n","        print(f\"Matriz de confusão salva em '{self.results_dir}/confusion_matrix.png'\")\n","\n","        # Relatório de Classificação\n","        predicted_classes = val_preds.argmax(dim=1)\n","        report = classification_report(val_labels.numpy(), predicted_classes.numpy(), target_names=self.class_names, digits=4, output_dict=True)\n","        report_df = pd.DataFrame(report).transpose()\n","        report_df.to_csv(f\"{self.results_dir}/classification_report.csv\")\n","        print(\"Relatório de classificação:\")\n","        print(report_df)\n","        print(f\"Métricas salvas em '{self.results_dir}/'\")\n","\n","    def show_insides(self, filter_layers=[], layers_to_hook=[]):\n","        # Visualização de filtros\n","        for layer in filter_layers:\n","            fig = self.arch.visualize_filters(layer)\n","            if fig:\n","                fig.savefig(f\"{self.results_dir}/filters_{layer}.png\")\n","                plt.close(fig)\n","                print(f\"Filtros da camada {layer} salvos em '{self.results_dir}/filters_{layer}.png'\")\n","\n","        # Pega um lote de imagens de validação\n","        images, labels = next(iter(self.val_loader))\n","        images_gpu = images.to(self.arch.device)\n","\n","        # Anexa os hooks\n","        self.arch.attach_hooks(layers_to_hook)\n","\n","        # Passa os dados pelo modelo para capturar as saídas\n","        with torch.no_grad():\n","            output = self.arch.model(images_gpu)\n","\n","        predicted_classes = output.argmax(dim=1).cpu().numpy()\n","\n","        # Adiciona as imagens originais para comparação no plot\n","        self.arch.visualization['images'] = images.cpu().numpy()\n","\n","        # Visualiza as saídas capturadas\n","        fig_outputs = self.arch.visualize_outputs(layers_to_hook, y=labels.numpy(), yhat=predicted_classes)\n","        fig_outputs.savefig(f\"{self.results_dir}/intermediate_outputs.png\")\n","        plt.close(fig_outputs)\n","        print(f\"Saídas intermediárias salvas em '{self.results_dir}/intermediate_outputs.png'\")\n","\n","        # Remove os hooks\n","        self.arch.remove_hooks()\n","\n","        # Mantém apenas os outputs textuais informando sobre o salvamento\n","        print(f\"\\nFiguras de filtros e ativações salvas em '{self.results_dir}/'\")\n","\n","    def run(self, layers, epochs=10, skip_train=False):\n","        if not skip_train:\n","            self.train(epochs)\n","\n","        checkpoint_path = f\"{self.results_dir}/model_checkpoint.pth\"\n","        self.arch.save_checkpoint(checkpoint_path)\n","        print(f\"\\nCheckpoint final salvo em: {checkpoint_path}\")\n","\n","        self.evaluate()\n","        self.show_insides(filter_layers=layers, layers_to_hook=layers)\n"],"metadata":{"id":"cNky5jJpy63Z","executionInfo":{"status":"ok","timestamp":1752149773595,"user_tz":180,"elapsed":6,"user":{"displayName":"Angelo Marcelino","userId":"12130434257979385717"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["# Tests"],"metadata":{"id":"VZKgOP6dkBHm"}},{"cell_type":"code","source":["example_dicts = [\n","    {\n","        \"in_channels\": 1,\n","        \"num_classes\": 10,\n","        \"input_size\": 28,\n","        \"amnt_cov_layers\": 3,\n","        \"cov_kernel_sizes\": [3, 3, 3],\n","        \"cov_features\": [8, 16, 32],\n","        \"max_pool_sizes\": [2, 2, 2],\n","        \"amnt_fc_layers\": 3,\n","        \"fc_sizes\": [100, 50],\n","        \"p\": 0.3\n","    },\n","]\n"],"metadata":{"id":"xOypj0MnkCT1","executionInfo":{"status":"ok","timestamp":1752149773610,"user_tz":180,"elapsed":13,"user":{"displayName":"Angelo Marcelino","userId":"12130434257979385717"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["model_generic_0 = GerenicCNN(**example_dicts[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LxMR9vefwaYb","executionInfo":{"status":"ok","timestamp":1752149773645,"user_tz":180,"elapsed":34,"user":{"displayName":"Angelo Marcelino","userId":"12130434257979385717"}},"outputId":"498a17dd-9f36-43af-8fb8-912cbc7d582e"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["conv0 - in_size: 28x28\n","\tin_channels: 1\n","\tout_channels: 8\n","\tkernel_size: 3\n","\tconv_out_size: 26x26\n","\tmaxpool_kernel_size: 2\n","\tout_size (after max pool): 13x13\n","conv1 - in_size: 13x13\n","\tin_channels: 8\n","\tout_channels: 16\n","\tkernel_size: 3\n","\tconv_out_size: 11x11\n","\tmaxpool_kernel_size: 2\n","\tout_size (after max pool): 5x5\n","conv2 - in_size: 5x5\n","\tin_channels: 16\n","\tout_channels: 32\n","\tkernel_size: 3\n","\tconv_out_size: 3x3\n","\tmaxpool_kernel_size: 2\n","\tout_size (after max pool): 1x1\n","fc0 - in: 32 - out: 100\n","fc1 - in: 100 - out: 50\n","fc2 - in: 50 - out: 10\n"]}]},{"cell_type":"code","source":["import torchvision\n","import torchvision.transforms as transforms\n","\n","# Download and load the training data\n","trainset = torchvision.datasets.FashionMNIST(root='./data', train=True,\n","                                        download=True,\n","                                        transform=transforms.ToTensor())\n","\n","# Download and load the test data\n","testset = torchvision.datasets.FashionMNIST(root='./data', train=False,\n","                                       download=True,\n","                                       transform=transforms.ToTensor())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QbcGyvucwcHD","executionInfo":{"status":"ok","timestamp":1752149780441,"user_tz":180,"elapsed":6790,"user":{"displayName":"Angelo Marcelino","userId":"12130434257979385717"}},"outputId":"2e4caa06-9218-43ea-e2c5-390162e6f1da"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 26.4M/26.4M [00:02<00:00, 12.8MB/s]\n","100%|██████████| 29.5k/29.5k [00:00<00:00, 200kB/s]\n","100%|██████████| 4.42M/4.42M [00:01<00:00, 3.76MB/s]\n","100%|██████████| 5.15k/5.15k [00:00<00:00, 15.6MB/s]\n"]}]},{"cell_type":"code","source":["train_loader = torch.utils.data.DataLoader(trainset, batch_size=16, shuffle=True)\n","val_loader = torch.utils.data.DataLoader(testset, batch_size=16, shuffle=False)"],"metadata":{"id":"imm3khFYzfV1","executionInfo":{"status":"ok","timestamp":1752149780444,"user_tz":180,"elapsed":2,"user":{"displayName":"Angelo Marcelino","userId":"12130434257979385717"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["mflow_generic_0 = ModelFlow(\n","    model=model_generic_0,\n","    train_loader=train_loader,\n","    val_loader=val_loader,\n","    model_name=\"generic_0\"\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KZzGro1DzXr_","executionInfo":{"status":"ok","timestamp":1752149780708,"user_tz":180,"elapsed":263,"user":{"displayName":"Angelo Marcelino","userId":"12130434257979385717"}},"outputId":"e30a7f6b-76e1-4b91-ee2b-12889d8ac171"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Diretório de resultados: 'results/generic_0'\n","Model sent to cuda\n","Architecture created\n","Loaders set\n","Train dataset size: 60000\n","Train batch size: 16\n","Modelo 'generic_0' - Parâmetros: 14748\n"]}]},{"cell_type":"code","source":["mflow_generic_0.run(\n","    layers=[f'conv_layers.{i}' for i in range(model_generic_0.amnt_cov_layers)],\n","    epochs=5\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_Eq3qP5YzafB","executionInfo":{"status":"ok","timestamp":1752149906576,"user_tz":180,"elapsed":125869,"user":{"displayName":"Angelo Marcelino","userId":"12130434257979385717"}},"outputId":"c23d1fc9-dca0-453e-c5ea-e488e4cc753d"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Iniciando treinamento do modelo...\n","Random seed set to 42\n","Starting training...\n","Epoch 1/5 | Train loss: 1.17549082 | Val. loss: 0.81845202 | Time: 19.53s | ETA: 1m 18s\n","Epoch 2/5 | Train loss: 0.84091184 | Val. loss: 0.69914543 | Time: 18.79s | ETA: 0m 57s\n","Epoch 3/5 | Train loss: 0.73942943 | Val. loss: 0.63505226 | Time: 20.13s | ETA: 0m 38s\n","Epoch 4/5 | Train loss: 0.69057940 | Val. loss: 0.59507751 | Time: 18.75s | ETA: 0m 19s\n","Epoch 5/5 | Train loss: 0.65248702 | Val. loss: 0.57403310 | Time: 19.01s | ETA: 0m 0s\n","Training completed in 96.21 seconds.\n","Mean time per epoch: 19.24 seconds.\n","Treinamento completo!\n","Checkpoint saved to results/generic_0/model_checkpoint.pth\n","\n","Checkpoint final salvo em: results/generic_0/model_checkpoint.pth\n","\n","Avaliando o modelo e salvando métricas...\n","Gráfico de perdas salvo em 'results/generic_0/losses.png'\n","Normalized confusion matrix\n","Matriz de confusão salva em 'results/generic_0/confusion_matrix.png'\n","Relatório de classificação:\n","              precision  recall  f1-score     support\n","T-shirt/top    0.763261  0.7770  0.770069   1000.0000\n","Trouser        0.978836  0.9250  0.951157   1000.0000\n","Pullover       0.646833  0.6740  0.660137   1000.0000\n","Dress          0.718750  0.8970  0.798043   1000.0000\n","Coat           0.604689  0.7480  0.668753   1000.0000\n","Sandal         0.961874  0.8830  0.920751   1000.0000\n","Shirt          0.533465  0.2710  0.359416   1000.0000\n","Sneaker        0.869105  0.9030  0.885728   1000.0000\n","Bag            0.917659  0.9250  0.921315   1000.0000\n","Ankle boot     0.895853  0.9290  0.912126   1000.0000\n","accuracy       0.793200  0.7932  0.793200      0.7932\n","macro avg      0.789032  0.7932  0.784749  10000.0000\n","weighted avg   0.789032  0.7932  0.784749  10000.0000\n","Métricas salvas em 'results/generic_0/'\n","Filtros da camada conv_layers.0 salvos em 'results/generic_0/filters_conv_layers.0.png'\n","Filtros da camada conv_layers.1 salvos em 'results/generic_0/filters_conv_layers.1.png'\n","Filtros da camada conv_layers.2 salvos em 'results/generic_0/filters_conv_layers.2.png'\n","Saídas intermediárias salvas em 'results/generic_0/intermediate_outputs.png'\n","\n","Figuras de filtros e ativações salvas em 'results/generic_0/'\n"]}]},{"cell_type":"code","source":["mflow_generic_0.run(\n","    layers=[f'conv_layers.{i}' for i in range(model_generic_0.amnt_cov_layers)],\n","    epochs=15\n",")"],"metadata":{"id":"rACNphLiz_Ch","executionInfo":{"status":"ok","timestamp":1752150529868,"user_tz":180,"elapsed":317801,"user":{"displayName":"Angelo Marcelino","userId":"12130434257979385717"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c9c5bf70-0bff-4d5f-97bc-eb379d9c7718"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Iniciando treinamento do modelo...\n","Random seed set to 42\n","Starting training...\n","Epoch 6/15 | Train loss: 0.62882615 | Val. loss: 0.56545760 | Time: 21.09s | ETA: 4m 55s\n","Epoch 7/15 | Train loss: 0.60691280 | Val. loss: 0.53464305 | Time: 19.19s | ETA: 4m 21s\n","Epoch 8/15 | Train loss: 0.58765984 | Val. loss: 0.52390974 | Time: 18.52s | ETA: 3m 55s\n","Epoch 9/15 | Train loss: 0.57081815 | Val. loss: 0.49904030 | Time: 19.11s | ETA: 3m 34s\n","Epoch 10/15 | Train loss: 0.55513643 | Val. loss: 0.50300020 | Time: 18.76s | ETA: 3m 13s\n","Epoch 11/15 | Train loss: 0.55057848 | Val. loss: 0.49701642 | Time: 19.13s | ETA: 2m 53s\n","Epoch 12/15 | Train loss: 0.54256808 | Val. loss: 0.48524596 | Time: 19.05s | ETA: 2m 34s\n","Epoch 13/15 | Train loss: 0.53261639 | Val. loss: 0.47768140 | Time: 18.97s | ETA: 2m 14s\n","Epoch 14/15 | Train loss: 0.52061234 | Val. loss: 0.46735828 | Time: 18.97s | ETA: 1m 55s\n","Epoch 15/15 | Train loss: 0.51039685 | Val. loss: 0.46772064 | Time: 18.63s | ETA: 1m 35s\n","Epoch 16/15 | Train loss: 0.50353125 | Val. loss: 0.45543044 | Time: 19.42s | ETA: 1m 16s\n","Epoch 17/15 | Train loss: 0.49811867 | Val. loss: 0.46076129 | Time: 18.57s | ETA: 0m 57s\n","Epoch 18/15 | Train loss: 0.48918124 | Val. loss: 0.44197638 | Time: 19.18s | ETA: 0m 38s\n","Epoch 19/15 | Train loss: 0.48302377 | Val. loss: 0.43148707 | Time: 19.46s | ETA: 0m 19s\n","Epoch 20/15 | Train loss: 0.47383060 | Val. loss: 0.43483866 | Time: 18.41s | ETA: 0m 0s\n","Training completed in 286.47 seconds.\n","Mean time per epoch: 19.10 seconds.\n","Treinamento completo!\n","Checkpoint saved to results/generic_0/model_checkpoint.pth\n","\n","Checkpoint final salvo em: results/generic_0/model_checkpoint.pth\n","\n","Avaliando o modelo e salvando métricas...\n","Gráfico de perdas salvo em 'results/generic_0/losses.png'\n","Normalized confusion matrix\n","Matriz de confusão salva em 'results/generic_0/confusion_matrix.png'\n","Relatório de classificação:\n","              precision  recall  f1-score     support\n","T-shirt/top    0.829689  0.7210  0.771536   1000.0000\n","Trouser        0.988554  0.9500  0.968893   1000.0000\n","Pullover       0.730873  0.8120  0.769304   1000.0000\n","Dress          0.841951  0.8630  0.852346   1000.0000\n","Coat           0.803279  0.6370  0.710541   1000.0000\n","Sandal         0.968652  0.9270  0.947368   1000.0000\n","Shirt          0.512903  0.6360  0.567857   1000.0000\n","Sneaker        0.900288  0.9390  0.919236   1000.0000\n","Bag            0.938939  0.9380  0.938469   1000.0000\n","Ankle boot     0.934132  0.9360  0.935065   1000.0000\n","accuracy       0.835900  0.8359  0.835900      0.8359\n","macro avg      0.844926  0.8359  0.838062  10000.0000\n","weighted avg   0.844926  0.8359  0.838062  10000.0000\n","Métricas salvas em 'results/generic_0/'\n","Filtros da camada conv_layers.0 salvos em 'results/generic_0/filters_conv_layers.0.png'\n","Filtros da camada conv_layers.1 salvos em 'results/generic_0/filters_conv_layers.1.png'\n","Filtros da camada conv_layers.2 salvos em 'results/generic_0/filters_conv_layers.2.png'\n","Saídas intermediárias salvas em 'results/generic_0/intermediate_outputs.png'\n","\n","Figuras de filtros e ativações salvas em 'results/generic_0/'\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"PLEsiQgHSnzf"},"execution_count":null,"outputs":[]}]}